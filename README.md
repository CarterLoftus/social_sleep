# social_sleep

This repository contains all the code needed to reproduce the results presented in Chapter 2 of my dissertation (The Physiology, Behavioral Ecology, and Collective Dynamics of Sleep in Wild Olive Baboons (Papio anubis)). The README presented here explains where the raw data is located and provides the instructions needed to repeat the data processing and analysis, allowing a reader to reproduce the results starting with only the raw data.

The raw infrared/thermal data used for the project is stored on the department storage for the Department for the Ecology of Animal Societies at the Max Planck Institute of Animal Behavior. The data is archived in the following directory: 'EAS_shared/baboon/archive/rawdata/video/thermal/2019_summer/cliff_data/thermal/viewpoint_1/T1020/'. Each subdirectory within this directory contains the raw data for each night of thermal data collected (and perhaps some other files too, but the .seq file is always the raw data file). The accelerometry and GPS data that was collected simultaneously is stored on Movebank under the project name: Papio Anubis Mpala 2019. Other data, as well as relevant documentation, that may be needed in reproducing the results are found within the folder ‘EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/’. 

To repeat the analyses, clone this repository to a local computer, and copy the contents of ‘EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/’ to the repository. The R scripts should run successfully without adjustment if they are run within the R project file (“social_sleep.Rproj”) that is within the repository. The python notebooks, however, might need minor file path adjustments so that they work on your local computer. Run the scripts in the order they are numbered to reproduce the results.

Annotation
The scripts “00a_writing_frames_for_annotation.ipynb”, “00b_writing_frames_for_burst_annotations.ipynb”, “00c_writing_full_vids_hard_drives.ipynb”, and “00d_writing_full_vids_from_server.ipynb” are all variations of code that take the raw .seq files, which are a in proprietary FLIR data format, and produce frames from the raw data. These frames are then stitched into videos (either directly in the script, or with instructions in the script about how to stitch the frames into videos in the command line). All of these scripts except for “00d_writing_full_vids_from_server.ipynb” require the raw data as an input to be on an external hard drive, as python is able to read the videos much faster from an external hard drive than directly from the data storage server.  “00a_writing_frames_for_annotation.ipynb” makes a video for each night that is comprised of 200 frames distributed evenly across the night (just extracts a subset for annotation purposes); “00b_writing_frames_for_burst_annotations.ipynb” makes a video for each burst of frames, with bursts of frames being distributed across the night (annotating bursts of frames was suggested by Ben Koger and Jake Graving); “00c_writing_full_vids_hard_drives.ipynb” creates thermal videos from the raw data for the full night, separated into 1 hour long (in real time) chunks. “00d_writing_full_vids_from_server.ipynb” does the same as the previous script, but reads the raw data directly from the department storage server (would not suggest using this script, because it takes prohibitively long to read the data from the storage server).

Either “00c_writing_full_vids_hard_drives.ipynb” or “00d_writing_full_vids_from_server.ipynb” have to be run at some point, because they produce the full videos for each night with a .mp4 file extension (again, separated into 1 hour long chunks), and these videos are the input to the tracking code (see “04_master_cv_tracking”). Before tracking, these videos need to be copied and pasted to this directory (in subdirectories, with the name of each subdirectory representing the date of the night of the videos contained): “EAS_shared/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/”.

The above scripts require the FLIR File SDK (Software Development Kit) to run successfully. Follow instructions here to setup the File SDK for Python: https://flir.custhelp.com/app/answers/detail/a_id/3504/kw/setup/track/AvMN_QqjDv8S~RHVGhEe~yJ7Ve8qVC75Mv95~zj~PP_w. If problems arise with installing or using the SDK, contact Justin Giaquinto at FLIR at Justin.Giaquinto@flir.com.

All of the above scripts produce videos that are either saved directly to the computer or to an external hard drive. I then uploaded these videos to Loopy (LoopBio GmbH, http://loopb.io/), and several research assistants and I used Loopy’s Annotation Tool functionality to annotate baboons in the videos. I downloaded the annotations from Loopy in both .json and .csv format and saved them here (again, on the EAS department storage): “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DATA/thermal_annotations /complete_thermal_annotations_2020_10_21/annotations”. (Note: I am missing some .csv files of annotations for 2019-07-31 and they are not recoverable, but I only actually used the .json files in further scripts, so this should not be an issue).

The scripts “01a_extracting_annotated_frames.ipynb” and “01b_extracting_annotated_frames_1.ipynb” extracts and prints the frames that were annotated, as determined by the downloaded annotations from Loopy. These frames were then moved to “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DATA/thermal_annotations/complete_thermal_annotations_2020_10_21/frames_matching_annotations”. The videos that were originally uploaded to Loopy for annotation (i.e. the videos from which the annotated frames were extracted) are saved here: “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DATA/thermal_annotations /complete_thermal_annotations_2020_10_21/videos_for_annotating”. They are separated into three different types of videos, that correspond to the three different types of videos produced for annotation as described above (in the first “Annotation” paragraph).
Note that there is a README file in the directory “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DOCS” that contains a detailed explanation of the annotations and their processing.

The script “02_convert_loopy_to_coco.ipynb” is a code written primarily by Ben Koger that converts the Loopy-formatted annotations in to COCO format. The script saves the COCO annotations and prints a subset of the annotations overlaid on their corresponding images just to confirm that the annotations and conversion of the annotations was done correctly.

Training model and tracking baboons in videos
The vast majority of this code in this section is written primarily by (or adapted from code written primarily by) Ben Koger. If you use the code within this section, please cite (in addition to this repository) the pre-print found here: https://www.biorxiv.org/content/10.1101/2022.06.30.498251v1.

For the next scripts, detectron2 needs to be installed. Follows these steps to install dectron2 (on Windows. To install on Linux, instructions are readily found on the internet):
1.	First uninstall everything:
a.	Go to control panel-Programs-Programs and features
b.	Search CUDA 
c.	Uninstall all NVIDIA CUDA
d.	Restart Computer
e.	CONDA uninstall pytorch
f.	Pip uninstall torch (In an anaconda shell)
g.	pip uninstall "git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI"
h.	Delete Detectron 2 (located at: ‘C:/Users/meerkat/detectron2-windows’
2.	Then install everything again (instructions for installation: https://dgmaxime.medium.com/how-to-easily-install-detectron2-on-windows-10-39186139101c)
a.	conda create -n myenv python=3.7
b.	conda activate myenv
c.	Install version 10.1 NVIDIA CUDA from this website: https://developer.nvidia.com/cuda-10.1-download-archive-base
d.	Restart computer
e.	Install torch with this line of code: conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch (I think in anaconda shell)
f.	Restart computer
g.	in Anaconda shell: pip install cython
h.	in Anaconda shell: pip install "git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI"
i.	git clone https://github.com/DGMaxime/detectron2-windows.git (was this in Git Bash or Anaconda shell?)
j.	in Anaconda shell: cd detectron2-windows
k.	In Anaconda shell: pip install -e .
l.	restart computer

Note that there are incompatibilities between the FLIR SDK and detectron2. I used two different computers, one that primarily interacted with the SDK and one that interacted with detectron2. It also would have been possible to process all FLIR videos with the SDK first before moving onto the detectron2 installation. Alternatively, it should be possible to solve this problem using virtual environments, but I did not have luck with this when I tried it.

The script “03_detection_training.ipynb” is code written primarily by Ben Koger. The script trains the convolutional neural network (CNN) model using the COCO annotations extracted and prepared in previous scripts. It produces a trained model which is stored in “social_sleep/DATA/detectron_output/”. If using this script, please cite this pre-print: https://www.biorxiv.org/content/10.1101/2022.06.30.498251v1.


The script “04_master_cv_tracking.ipynb” is code written primarily by Ben Koger. This script processes the full thermal videos (that have been converted from the .seq proprietary data format to .mp4 videos; see “00c_writing_full_vids_hard_drives.ipynb” or “00d_writing_full_vids_from_server.ipynb”) that have been chunked into hour long chunks and saved in “EAS_shared/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/”. The script applies the model that was trained and saved in the previous script to each frame of a thermal video to extract detections of the baboons in the video, and then uses the tracking method to string together the detections into continuous baboon tracks for the video. The script does this for each thermal video. This script produces detections (bounding box predictions of where baboons are on each frame), positions (center points of detections), and tracks (both .npy and .csv file formats), as well as a video of the tracks overlaid on the input video, for each video, and saves these data in “social_sleep/DATA/tracking_output”. If using this script, please cite the following pre-print: https://www.biorxiv.org/content/10.1101/2022.06.30.498251v1.

To determine appropriate hyperparameters in the previous script (“04_master_cv_tracking.ipynb”), I ran the script on an example video repeatedly, varying one parameter at a time. I evaluated performance with visual inspection of the video with overlaid tracks. The excel sheet that I used to guide the parameter search and where I noted the final parameters used is located here: “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DOCS/parameter_search.xlsx”. The row indicating “parameters_used_for_tracking_more_tracks_parameters” represents the parameters used for extracting the positions of baboons at particular times of the night that were used in the analysis (note, even though these parameters include all parameters for tracking as well as detection, only the positions (the center point of detections) were used in further analysis). The row indicating “parameters_used_for_tracking_fewer_tracks_parameters” represents the parameters used for tracking (and thus, analyses that involved continuous tracks).

I therefore ran the code “04_master_cv_tracking.ipynb” once with the parameters that I used to determine the positions of baboons at various points in the night, and copied and pasted the output into a subdirectory within “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DATA/tracking_output” that I titled “more_tracks_parameters”. I ran the code again with the parameters that I used for continuous tracking, and copied and pasted the output into a subdirectory within “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DATA/tracking_output” that I titled “fewer_tracks_parameters”.

The script “05_precision_recall_curves.ipynb” then determines and plots the precision-recall curves for the detection model, using the two different parameter settings.

We manually corrected and stitched the tracks saved in “social_sleep/DATA/tracking_output/fewer_tracks_parameters” to produce the most accurate tracks possible. To do this, we first decomposed the videos (whose tracks we were manually correcting) to individual frames using the script “06_decompose_video_to_frames.ipynb”. This script takes an input video (the video whose tracks the user is currently going to correct) and breaks it into individuals frames and saves these frames in a folder “social_sleep/DATA/frames_for_stitching/”. (The protocol used to manually stitch the tracks, as well as to run the code for manual stitching can be found here: “social_sleep/DOCS/Manual Track Correction Protocol.docx” and “social_sleep/DOCS/local_correction_GUI_protocol.docx”).

The script “07_track_correction_GUI.ipynb” creates a graphical user interface that allows a user to manually correct the tracks produced by “04_master_cv_tracking.ipynb”. This script takes the .npy tracks file to edit and the folder containing the corresponding frames (produced in “06_decompose_video_to_frames.ipynb”) as inputs. Upon pressing “Esc”, the script saves an updated tracks file in the directory with the input tracks file, with a “-final” suffix. 

The script “08_tracks_to_video.ipynb” takes a tracks file and input video (or folder of frames from an already decomposed video) and produces a video with overlaid tracks. I used this script to produce videos with the manually corrected tracks overlaid to visually assess whether the manual corrections were sufficient, or whether there were some corrections that had been missed. If there were corrections that had been missed, the research assistants or I used the updated tracks file as the new input for the previous script to continue making corrections in graphical user interface. When the tracks had been perfectly corrected, we moved on to the next script.

The script “09_finalize_tracks.ipynb” takes a corrected tracks file as an input and copies this file over to a folder of finalized tracks files (“social_sleep/DATA/tracking_output/finalized_tracks/”). It also produces a .csv file of the tracks file (as the output of the “07_track_correction_GUI.ipynb” script is a .npy file), as well as a video of the tracks overlaid on the video. Lastly, it copies the .txt file that provides the timestamp associated with each frame of the relevant video to the finalized tracks folder. All of this is saved within a subdirectory of the finalized_tracks folder that is titled with the date of the night of the data. (NOTE: the manual corrections of tracks is an ongoing process that continued during and beyond the analysis of data for my dissertation. Thus, the tracks that are in the directory “EAS_shared/baboon/archive/pubs/Loftus_dissertation/social_sleep/DATA/tracking_output/finalized_tracks/” represents that final tracks that I used for analysis for my dissertation.)

The script “10_positions_npy_to_csv.ipynb” goes through all subdirectories of “social_sleep/DATA/tracking_output/more_tracks_parameters” (i.e. each night of data, as each subdirectory represents a night) and produces a .csv file of the positions for each “positions*.npy” file. These .csv files are later loaded into R and used for the analysis of the spatial positioning of group members at a particular time(s) of the night. The rest of the data (i.e. the continuous tracks) from the “social_sleep/DATA/tracking_output/more_tracks_parameters” directory is not used in the analysis.

Identifying individuals in thermal imagery and processing behavioral observations
We identified individuals in the thermal imagery so that we could validate the sleep algorithm (see below). This identification was completed using two methods. 

The first method (method 1) involved real-time narration of “normal” (i.e. not thermal) videos that were taken simultaneously with thermal videos. These normal videos can be found in this directory: “EAS_shared/baboon/working/video/feeding/videos”. Within the folder for each day inside this directory, the subdirectory “PM/Carter” contains the real-time narration (if there is real-time narration for that day). The file “social_sleep/DOCS/all_modes_meta_data.csv” contains the information indicating on which days real-time narration was completed. The file “social_sleep/DOCS/all_modes_meta_data_README.docx” explains the information contained in the file “social_sleep/DOCS/all_modes_meta_data.csv”. This method of individual identification is described in detail in “social_sleep/DOCS/Thermal identification protocol.docx”

The second method (method 2) consisted of matching baboons in the thermal imagery to their GPS tracks that were overlaid on a background drone image of the cliff (the main sleep site where the study group slept). Note, the thermal videos and the videos of the GPS tracks were separate videos for method 2. Roi Harel created the videos of the GPS tracks overlaid on the drone image of the cliff. These videos were stored in the Google Drive “Baboon_coding_Mpala_2019” (owned by Roi Harel), but they have since been removed.

The third method (method 3) was similar to method 2, in that baboons in the thermal imagery were identified based on visualizing their sensor data. In method 3, however, the thermal imagery with the final track identities (after manual track stitching) overlaid were combined directly in one video with the GPS tracks overlaid on the drone imagery of the cliff and a display of the accelerometry data. Because it was hard to visualize the accelerometry data of more than one baboon at a time, we created separate videos (each night separated into an hour’s worth of data, as for the thermal video processing above) for each collared individual. These videos were produced on the MPCDF HPC (Raven) using the following methods:
1.	First, we used the script “11_cropping_cliff_tiff.R” to crop the drone imagery of the baboon’s main sleep site to the area covered by the thermal video (original drone imagery of Mpala captured by Blair Costelloe; the relevant tile is stored at “social_sleep/DATA/cliff_tiff/ebee_highdam_transparent_mosaic_group1.tif”). This script takes “social_sleep/DATA/cliff_tiff/ebee_highdam_transparent_mosaic_group1.tif” as an input and produces “social_sleep/DATA/cliff_tiff/cropped_cliff.tif” and “social_sleep/DATA/cliff_tiff/less_cropped_cliff.tif” as outputs.
2.	We downloaded the GPS and accelerometry (ACC) data from Movebank, which is under the project name: Papio Anubis Mpala 2019. This downloaded csv (downloaded with all sensor types) was then moved to the directory “social_sleep/DATA/gps_acc_data/”
3.	We then used the script “12_prepping_GPS_and_ACC.R”, which takes “social_sleep/DATA/gps_acc_data/Papio Anubis Mpala 2019.csv” and “social_sleep/DATA/Collaring_data.csv” as input, and removes data from baboons that were not in the main study group, and trims the data to the relevant columns. It also separates the GPS and ACC data, and applies a time correction so that it is time-matched with the thermal data. It outputs the GPS and ACC data as “social_sleep/DATA/gps_acc_data/gps_dat.csv” and “social_sleep/DATA/gps_acc_data/acc_dat.csv”, respectively
4.	We used the script “13_parsing_acc_for_acc_gps_thermal_for_ID” to parse the ACC data (i.e. to convert it from one row per burst to one row per sampling time”) in preparation for plotting. The script takes “social_sleep/DATA/gps_acc_data/acc_dat.csv” as an input, as well as the .txt files with the timestamps associated with each frame for each video that are stored here: 'EAS_shared/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/'. It then produces the directory “social_sleep/DATA/gps_acc_data/parsed_acc_for_animation/” and a subdirectory for each night, and within each night, a subdirectory for each hour of the thermal video. The ACC data that occurs within that hour is then saved as a separate .csv file for each individual baboon.
5.	We used FileZilla to transfer the “social_sleep” directory, using the same directory structure, to the Max Planck Computing and Data Facility (MPCDF) high-performance computing (HPC), “Raven” (the whole directory does not necessarily need to be transferred: but the less_cropped_cliff.tiff and gps_dat.csv files need to be transferred as well as the finalized_tracks, parsed_acc_for_animation, and CODE directories).
a.	 Download and open FileZilla
b.	VPN into the institute network
c.	Click File  Site Manager
d.	Click “New Site” and then under Protocol, select “SFTP – SSH File Transfer Protocol”; under Host, enter “gateafs.rzg.mpg.de”; and under Port, enter “22” (not sure the number is actually important). Select “Interactive” for Logon type, and then enter the username (cloftus, for me)
e.	Drag and drop “social_sleep” (or the relevant directories within, in the proper structure) from the local computer to “/afs/ipp-garching.mpg.de/home/c/cloftus” (the default folder after login
f.	Download and open Putty
g.	Enter “cloftus@gateafs.rzg.mpg.de” under Host Name, and “22” under Port. Press “Open”
h.	Enter password and OTP
i.	Enter “ssh raven” and press Enter
j.	Enter “cp -rf /afs/ipp-garching.mpg.de/home/c/cloftus/social_sleep /raven/ptmp/cloftus/” and press Enter. This will copy the files that were copied to gateafs (with FileZilla) to Raven
k.	(if connected directly to the MPI network, can enter “cobra.mpcdf.mpg.de” as Host in FileZilla, and then enter “/raven/ptmp/cloftus/” under “Remote site” to transfer the files directly to Raven and skip the Putty steps)
6.	Create the python environment needed to run the code on Raven
a.	Go to “https://rvs.mpcdf.mpg.de/”; click “Start Here”. Log in.
b.	Click “Initialize Remote Visualization”
c.	Choose Machine: “RAVEN”, enter password
d.	Click “Submit new session”
e.	Choose Machine: “RAVEN”; Session Type: “Jupyter”; Session length: 8 hours (or however long you need); Interface: “Classic”; Software: “Python 3.7”. Click “Submit Session”; “Back to Main Page”
f.	Click “Connect to Session”, “Show Sessions”; “connect”. Enter password
g.	In Jupyter Notebook’s file explorer, navigate to “ptmp_link”, “social_sleep”, “CODE”
h.	Open a new terminal from Jupyter Notebook. Run the following lines:
i.	cd /raven/ptmp/cloftus/thermal_baboon/CODE/
ii.	conda create --name thermal_acc_gps_virtualenv_37 python=3.7
iii.	conda activate thermal_acc_gps_virtualenv_37
iv.	conda install -c conda-forge gdal
v.	conda install -c conda-forge rasterio
vi.	conda install pytorch torchvision torchaudio -c pytorch 
1.	the terminal needs to be closed and reopened
2.	tips to get as to this point taken from here: https://opensourceoptions.com/blog/how-to-install-gdal-with-anaconda/
i.	reopen a terminal, and install the following modules, again before opening a notebook
i.	cd /raven/ptmp/cloftus/thermal_baboon/CODE/
ii.	conda activate thermal_acc_gps_virtualenv
iii.	conda install pandas tqdm sortedcontainers joblib
iv.	pip install decord opencv-python matplotlib
j.	if planning to run the actual code using jupyter notebook (can be helpful to test the code before submitting the full job), run the following code to make the environment accessible as a kernel in jupyter notebook (these next tips taken from here: https://stackoverflow.com/questions/66359821/conda-environment-and-jupyter-notebook-python-3-9-2)
i.	conda install -c conda-forge ipykernel
ii.	python -m ipykernel install --user --name thermal_acc_gps_virtualenv_37 --display-name "Python 3 (thermal_acc_gps_37)"
iii.	then close down jupyter notebook and open it back up again the environment should be an option in the kernel tab
7.	Submit a job for the script “14b_acc_gps_thermal_for_ID_python_3_7_for_HPC_parallel_a.py”
a.	Open Putty
b.	Enter “cloftus@gateafs.rzg.mpg.de” under Host Name, and “22” under Port. Press “Open”
c.	Enter password and OTP
d.	Enter “ssh raven”
e.	Enter “cd /raven/ptmp/cloftus/social_sleep/CODE”
f.	Enter “sbatch therm_acc_GPS_parallel_a.slrm”
g.	Enter “squeue --me” to see progress of jobs
h.	“14b_acc_gps_thermal_for_ID_python_3_7_for_HPC_parallel_a.py” is just an example script that will create videos for the whole night for the first half of the individuals for the night 2019-08-06. Change ‘night’ variable to run a different night, change ‘inds’ variable to process particular videos within that night, and ‘tag_names’ subsetting to process videos for particular individuals. Making this subsetting more restrictive will reduce the memory needed, and the memory can be decreased accordingly to give the job higher priority in the slurm processing (there are only a few nodes with 2 TB of memory, which this script will require). To decrease the memory allocated for the job, just delete the “mem” line in the slrm script
i.	After the job has finished, the resulting videos can either be transferred back to the local computer with Putty and FileZilla (going in the opposite direction as described above), or by using the Remote Desktop on Raven (Submit new session at rvs.mpcdf.mpg.de/rv/) to upload the videos to Google Drive from the green circle icon  Desktop Apps  Other  Files using Firefox (green circle icon  Desktop Apps  Network  Firefox)
Vilson Karaj performed methods 1 and 2 and Arianna Flood performed methods 1 and 3. I also performed method 3 shortly (during confirmation – see below). The resulting identifications made from these methods are stored in the folder “social_sleep/DATA/identification_in_thermal”. These identifications were used to perform focal follows of identified individuals in the thermal imagery. I used method 3 to confirm identifications made (primarily on the night of 2019-08-06) and saved these confirmed identity matches between finalized tracks and individuals (as determined by their collar data) in the file “social_sleep/DATA/identification_in_thermal/final_id_and_focal.csv”. These confirmed matches do not include all individuals that were identified for focals (as not we did not need to know the track identity of all individuals for the focal follows; as long as we knew their collar identity, that was sufficient for the sleep validation study. Thus, on nights in which the manual corrections of tracks were not completed, we don’t have a match between track identity and collar identity, even if an identification in the thermal was made). These confirmed matches may be helpful to developing an algorithm to automatically match thermal tracks to a collar identity (and they are necessary for the script “19_adding_stat_nonstat_label.R” to run).

The script “15_prepping_therm_focals.R” takes the output of the focal follows that were completed in Loopy and Boris, and reformats them so that they can easily be used to validate the sleep classification algorithm and train the machine-learning algorithm that determines whether individuals are moving or stationary in the thermal imagery (see “19_adding_stat_nonstat_label.R”). It takes the files from Loopy and Boris that are contained within respective subdirectories within the directory “social_sleep/DATA/thermal_focal_follows/” as an input (which is where I downloaded the files to upon downloading them from Loopy and Boris after completing focal follows), and outputs “social_sleep/DATA/thermal_focal_follows/sec_focal_dat.csv”.

Thermal track processing and analysis
The script “16_merging_tracklets_across_videos.R” takes either the uncorrected tracks (raw output of thermal tracking, with the fewer tracks parameters: “social_sleep/DATA/tracking_output/fewer_tracks_parameters”) or the manually corrected tracks (“social_sleep/DATA/tracking_output/finalized_tracks”) – depending on the user-input (note that the script should eventually be run with both inputs) – and merges tracklets that exist at the end of one video within a night and to the closest tracklets that are nearby that start at the beginning of the next video within the night. This results in one set of complete tracks for the entire night, with a unique individual identifier for each track within the night. This output is saved as .rds files (one per night) within subdirectories (for uncorrected and corrected tracks) within the directory “social_sleep/DATA/thermal_tracks/auto_stitch_across_vids/”.

We then performed a spatial correction of the tracks to account for the angled perspective from which the video was taken. We used ImageJ to measure a uniform object as it was moved across the video field of view, and to plot points along the horizontal ledge of the cliff. The measurements are saved in the file “social_sleep/DATA/thermal_spatial_correction/spatial_correction.csv” and the cliff coordinates are saved in the file “social_sleep/DATA/thermal_spatial_correction/coords_along_cliff.csv”. These files are used as inputs, along with the tracks in “social_sleep/DATA/thermal_tracks/auto_stitch_across_vids/” for the script “17_track_spatial_correction.R”, which applies the spatial correction. The spatially corrected tracks are then saved as .rds files within the directory “social_sleep/DATA/thermal_tracks/spatially_corrected_tracks/”. Note that, as above, this code should be run for both the “corrected” and “uncorrected” tracks.

The script “18_smoothing_thermal_tracks.R” takes the tracks saved in “social_sleep/DATA/thermal_tracks/spatially_corrected_tracks/” as an input and smooths them by taking the median x and y location for each second (decreasing the sampling rate to 1 Hz; i.e. the script does not smooth with a rolling window). This script also calculates the speed of the smoothed tracks. The output is then saved as .rds files within the directory “social_sleep/DATA/thermal_tracks/smooth_tracks/”. Note that, as above, this code should be run for both the “corrected” and “uncorrected” tracks.

The script “19_adding_stat_nonstat_label.R” takes the tracks files in the directory “social_sleep/DATA/thermal_tracks/smooth_tracks/” as an input, as well as the processed focal follow data stored at “social_sleep/DATA/thermal_focal_follows/sec_focal_dat.csv” and trains a machine learning model to distinguish between stationary and non-stationary behavior based on the speeds of the tracks. The trained model is saved at “social_sleep/DATA/thermal_tracks/stat_nonstat_ml_model.rds”. This model is then used to apply a label of stationary or non-stationary to each track at each second in the second part of the script. If running the second part of the script for the corrected tracks, the first part of the script (the part that prepares the data and trains the model) needs to be run first. After running the script on the corrected tracks, just the second part of the script needs to be run again for the uncorrected tracks (this is clarified within the script). The script then saves the data frames with the additional column for stationary vs. non-stationary (and an additional column for the manually confirmed identities of tracks, for the corrected tracks) by overwriting the .rds files in the “social_sleep/DATA/thermal_tracks/smooth_tracks/” directory.

The script “20_nocturnal_movement_synchronization_analysis.R” tests for synchronized nocturnal movements across group-mates during the night using a time-shifting permutation approach. The script takes the tracks within “social_sleep/DATA/thermal_tracks/smooth_tracks/” as an input, and runs the analysis, tests for significance, and plots the results, producing Figures 2.1A and 2.1B of the dissertation. To reproduce the results of the dissertation, the input_data variable should be set to “uncorrected”.

The script “21_spatial_propagation_of_activity.R” takes the corrected, smoothed tracks (saved here: "social_sleep/DATA/thermal_tracks/smooth_tracks/corrected") as an input and tests whether baboons that become active during the night cause more group-mates to become active in the proximity of where they become active or upon moving to other locations within the sleep site. The script performs a Wilcoxon signed rank test to test for statistical significance. 

The script “22_processing_thermal_positions.R” takes the position .csv files (i.e. the detections, but not the tracks, output by the detection algorithm) produced by “10_positions_npy_to_csv.ipynb” and stored in “social_sleep/DATA/tracking_output/more_tracks_parameters”, and combines all detections within each night into a single data frame for each night, associates a timestamp with each detection, and applies the spatial correction to the detections. It then outputs this information in .rds files for each night within the directory “social_sleep/DATA/thermal_tracks/raw_positions_processed/”.

The script “23_spatial_network_measures.R” takes the positions within the directory “social_sleep/DATA/thermal_tracks/raw_positions_processed/” as an input and extracts measures of the spatial network at user-input intervals throughout each night. This information is then saved in “social_sleep/DATA/thermal_tracks/spatial_net_measures_positions.csv”. Note that this code only needs to be run with the input_data variable set to “positions”.

The script “24_plotting_number_of_inds_moving.R” takes the files in "social_sleep/DATA/thermal_tracks/smooth_tracks/uncorrected” as an input and creates Figure S2.1

Sleep analysis
The script “25_trimming_acc_data_2019.ipynb” takes “social_sleep/DATA/gps_acc_data/Papio Anubis Mpala 2019.csv” as an input and down-samples and interpolates the different ACC sampling schedules such that they match across the whole day and across collars. It outputs this down-sampled and interpolated data in the data frame “social_sleep/DATA/sleep_analysis_data/all_burst_acc.csv”

The script “26_calculating_log_VeDBA_of_bursts.R” takes “social_sleep/DATA/sleep_analysis_data/all_burst_acc.csv” as an input and removes the data from individuals that were not members of the main study group. The script then calculates the mean of the log VeDBA over each burst (after the ACC data was down-sampled and interpolated in the last script such that all bursts are identical). The data with the mean log VeDBA for each burst included is output in the data frame “social_sleep/DATA/sleep_analysis_data/processed_ACC_emom.csv”

The script “27_sleep_classification_algorithm.R” takes “social_sleep/DATA/sleep_analysis_data/processed_ACC_emom.csv” and “social_sleep/DATA/thermal_focal_follows/sec_focal_dat.csv” as inputs and both classifies sleep from the accelerometry and validates the sleep classification algorithm based on the behavioral observations of identified individuals in the thermal imagery. The script prints the confusion matrix and outputs “social_sleep/DATA/sleep_analysis_data/full_dat_percentile_thresh_sep_thresh_TRUE.csv”, which contains the minute-by-minute sleep classification, and “social_sleep/DATA/sleep_analysis_data/sleep_per_percentile_thresh_sep_thresh_TRUE.csv”, which contains the nightly sleep summaries.

The script “28_formatting_social_sleep_data.R” takes “social_sleep/DATA/sleep_analysis_data/full_dat_percentile_thresh_sep_thresh_TRUE.csv” (the minute-by-minute sleep data) as an input and puts it into a format that is more conducive to social analyses (one row per timestamp of sleep data collection, i.e. per minute, and one column per individual). This reformatted data is output in the file “social_sleep/DATA/sleep_analysis_data/total_dat.rds”. 

The script “29_sleep_synchronization_analysis.R” takes “social_sleep/DATA/sleep_analysis_data/total_dat.rds” as an input and tests for synchronization in the sleep-wake patterns of group-mates during the night. The script also plots the results of the analysis.

The script “30_wakefulness_propagation_analysis.R” takes “social_sleep/DATA/sleep_analysis_data/full_dat_percentile_thresh_sep_thresh_TRUE.csv” as an input and runs a model to test how the number of individuals that awoke in the previous minute influences the likelihood a focal baboon waking up in the current minute. The script plots the results of this model, which represent Figure 2.1C in my dissertation.

The script “31_transfer_entropy_analysis.R” takes “social_sleep/DATA/sleep_analysis_data/total_dat.rds” as an input and performs a time-shifting permutation procedure to test whether the amount of transfer entropy between group-mates’ sleep states is higher than expected by chance. The script performs the permutations, tests for statistical significance, and produces Figure 2.1D of the dissertation. In addition, this script calculates the transfer entropy network that is used to test for correlations with the social networks (see “32_transfer_entropy_vs_social_networks.R”) and saves this transfer entropy network as “social_sleep/DATA/social_data/agg_te_net.rds”.

The script “32_transfer_entropy_vs_social_networks.R” takes “social_sleep/DATA/social_data/agg_te_net.rds”, “social_sleep/DATA/social_data/cosit_2019.RData”, and “social_sleep/DATA/social_data/Displace.csv” as inputs. Both were “social_sleep/DATA/social_data/cosit_2019.RData” and “social_sleep/DATA/social_data/Displace.csv” were obtained from Roi Harel and placed in the directory “social_sleep/DATA/social_data/”. This script tests for correlations between the social networks (affiliative and dominance) and the sleep state transfer entropy network, with both MRQAP (network statistics) and a hierarchical linear model. The script fits the models and plots the results to produce Figures 2.2B and 2.2C in the dissertation.

The script “33_network_based_diffusion_analysis.R”, and the supporting functions that are loaded into this script were primarily written by Sonja Wild. This script takes “social_sleep/DATA/sleep_analysis_data/total_dat.rds”, “social_sleep/DATA/social_data/cosit_2019.RData”, and “social_sleep/DATA/social_data/Displace.csv” as inputs and performs the network-based diffusion analysis to assess the role of the social networks (affiliative and dominance) in guiding the propagation of wakefulness during the night. The code is run once with ties_input set equal to TRUE and five separate times with ties_input set equal to FALSE to produce Tables S2.3 and S2.4 in the dissertation.

The script “34_total_sleep_time_vs_social_and_spatial_networks.R” takes “social_sleep/DATA/sleep_analysis_data/sleep_per_percentile_thresh_sep_thresh_TRUE.csv”, “social_sleep/DATA/social_data/cosit_2019.RData”, “social_sleep/DATA/social_data/Displace.csv”, “social_sleep/DATA/Collaring_data.csv”, and “social_sleep/DATA/thermal_tracks/spatial_net_measures_positions.csv” as inputs. The script first models the influence of dominance and social centrality in the affiliative network on total time spent sleeping. The script plots the results of this model, which represent Figures 2.4A and 2.4B in my dissertation. The script then models the influence of the density and modularity of the spatial network (at 22:00:00 on a given night) on the total time spent sleeping on that given night. The script plots the results of this model, which represent Figures 2.3A and 2.3B in the dissertation.

The script “35_visualizing_networks.R” creates dynamic networks that are used in presentation to show the spread of wakefulness during the night within the group, as well as multiplex networks that show the group’s dominance and affiliative networks, along with the sleep state transfer entropy network (i.e. Figure 2.2A in the dissertation).

Note: This repository used to be called thermal_baboon. It is possible that in some of the python code I forgot to replace “thermal_baboon” with “social_sleep”. If that is the case, simply replace “thermal_baboon” with “social_sleep” so that the code runs successfully.
