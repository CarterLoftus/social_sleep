{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: decord in c:\\users\\meerkat\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\meerkat\\anaconda3\\lib\\site-packages (from decord) (1.17.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\meerkat\\anaconda3\\lib\\site-packages (4.50.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install detectron2 -f \\\n",
    "  https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "from read_metadata_HIWI import read_metadata\n",
    "import subprocess\n",
    "from run_inference_HIWI import run_inference, extract_frames, video_to_frames, get_gpu_memory\n",
    "from export_tracks import export_tracks, calc_distance, get_color\n",
    "from detections_to_tracks import detections_to_tracks\n",
    "import pandas as pd \n",
    "import tqdm\n",
    "import numpy as np\n",
    "''\n",
    "import os\n",
    "from decord import VideoReader, cpu, gpu\n",
    "import cv2\n",
    "import subprocess as sp\n",
    "import tqdm\n",
    "\n",
    "import torch \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## the name of a video to perform tracking in without video without the file extension\n",
    "# # this just has to be any video on the desired date of tracking. In reality, we will loop through all videos on this data and track within them\n",
    "# # really, all we need here is the name of the folder that contains vids we want to use for tracking. I will come back and make it so we can either select a single vid, or just name the whole folder up front\n",
    "# vid_name = '20190806_142005603000'\n",
    "\n",
    "# ## declare the trained model you want to use for the tracking. Just need the folder name here (not the .pth file)\n",
    "# model_name = 'one_class_new_maxiter-2000_lr-0.019_detectPerIm-200_minsize-0_batchsize-8'\n",
    "\n",
    "# ## extract the date of the video from the video name. This is relevant due to the file structure of how the videos are stored\n",
    "# vid_date = vid_name[ :8 ]\n",
    "# ### rename 's/\\.MP4$/\\.mp4/' *.MP4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead of previous cell\n",
    "\n",
    "# save the folder (highest level) containing all videos to be processed\n",
    "input_video = 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/'\n",
    "\n",
    "# declare the trained model you want to use for the tracking\n",
    "model_file = \"C:/Users/meerkat/Documents/social_sleep/DATA/detectron_output/output/one_class_new_maxiter-2000_lr-0.019_detectPerIm-200_minsize-0_batchsize-8/model_final.pth\"\n",
    "\n",
    "# provide a file path for the temporary images that will be created from the videos\n",
    "images_folder = \"C:/Users/meerkat/Documents/social_sleep/DATA/images_to_process/\"\n",
    "\n",
    "# provide a file path for the output folder (this doesn't have to exist yet; it will be created below if it doesn't currently exist)\n",
    "output_folder = \"C:/Users/meerkat/Documents/social_sleep/DATA/tracking_output/\"\n",
    "\n",
    "# provide the folder that contains the python modules for import\n",
    "code_folder = \"C:/Users/meerkat/Documents/social_sleep/CODE/inference_to_tracking/\"\n",
    "\n",
    "# provide the folder where detectron2 is saved\n",
    "detectron_folder = \"C:/Users/meerkat/detectron2-windows/\" \n",
    "\n",
    "# give the file extension of the videos to be processed\n",
    "vid_file_ext = 'mp4'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/\n",
      "C:/Users/meerkat/Documents/thermal_baboon/RESULTS/detectron_output/output/one_class_new_maxiter-2000_lr-0.019_detectPerIm-200_minsize-0_batchsize-8/model_final.pth\n"
     ]
    }
   ],
   "source": [
    "## what do these do?\n",
    "# %store vid_name\n",
    "# %store -r vid_name\n",
    "\n",
    "# metadata = read_metadata( vid_name ) ## expanded below\n",
    "\n",
    "### the only thing actually needed from this whole cell is the bit that sets the folder location where the videos for tracking are (metadata.folder_data_server)\n",
    "class struct:\n",
    "    pass\n",
    "\n",
    "## instantiate metadata\n",
    "metadata = struct()\n",
    "\n",
    "## this is the repository -- the main folder the project is occuring within\n",
    "#metadata.folder_main = \"C:/Users/meerkat/Documents/thermal_baboon/\"\n",
    "\n",
    "## this is the path to where detectron2 itself is installed on the computer\n",
    "\n",
    "metadata.folder_detectron = detectron_folder\n",
    "\n",
    "\n",
    "## this is where the video is where the tracking will occur\n",
    "metadata.folder_data_server = input_video\n",
    "\n",
    "## this is the path to the model ouput .pth file (currently trained in baboon_detection_training.ipynb)\n",
    "metadata.baboon_weights = model_file\n",
    "\n",
    "## this will use a folder that has all the images that will be turned into the video\n",
    "metadata.folder_images = images_folder\n",
    "\n",
    "## this will make a folder with the output of the video\n",
    "metadata.folder_output = output_folder\n",
    "\n",
    "## this saves the video name. Not needed?\n",
    "#metadata.videoname = vid_name\n",
    "\n",
    "## here is the folder that contains this code (need this in order to import the functions and modules that Ben has written)\n",
    "metadata.folder_code =  code_folder\n",
    "\n",
    "## I don't know what this is for\n",
    "#metadata.folder_annotations =  metadata.folder_data +  'annotations/'\n",
    "\n",
    "\n",
    "print( metadata.folder_data_server )\n",
    "print( metadata.baboon_weights )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### extracting frames from input video #######\n",
    "frames_already_extracted = False\n",
    "delete_frames = True\n",
    "\n",
    "# enter the interval of frames from the input videos that you would like to extract (interval = 1 means each frame will be used for tracking)\n",
    "image_interval = 1 ## image_interval is the step length between each frame to extract from the video. every = 1 pulls out every frame. every = 2 pulls out every other frame, etc.\n",
    "\n",
    "## set the start and end frames to be extracted from the input video. This makes it so we can limit the frames on which we do tracking to a certain part of the video. -1 means the whole video\n",
    "start_init = -1\n",
    "end_init = -1\n",
    "\n",
    "## overwrite determines whether to overwrite the files that are already in the images folder\n",
    "overwrite = True\n",
    "\n",
    "\n",
    "###### detection hyperparameters ######\n",
    "\n",
    "max_batches = 150000 ## ## hyperparameter this cuts off the predicting after n batches, and because there are two frames in each batch, this makes the max video size n*2 frames (this parameter can be changed to limit predictions, but better to limit that by extracting fewer frames from the input video, because limiting with max_batches will drop pieces of the video at random (i.e. it won't be continuous))\n",
    "    \n",
    "### see cfg hyperparameters below\n",
    "\n",
    "BATCH_SIZE_PER_IMAGE = 512\n",
    "NUM_CLASSES = 1\n",
    "DETECTIONS_PER_IMAGE = 80\n",
    "BASE_LR = 0.005\n",
    "MAX_ITER = 4000\n",
    "SCORE_THRESH_TEST = 0.8\n",
    "NMS_THRESH_TEST = 0.35\n",
    "\n",
    "###### Tracking hyperparameters ######\n",
    "\n",
    "score_thresh = 0.1\n",
    "max_distance_threshold = 100\n",
    "max_distance_threshold_noise = 100\n",
    "min_distance_threshold = 10\n",
    "max_unseen_time = 30\n",
    "min_new_track_distance = 0\n",
    "min_length_threshold = 1\n",
    "noise_thresh = 11\n",
    "\n",
    "##### Plotting tracks hyperparameters #####\n",
    "steps = 10  # 90 # determines the number of timesteps into the past (with each frame = 1 timestep) the tail should be printed\n",
    "step_inc = 1 # 5 # determines the timestep interval at which a point for the tail is actually printed\n",
    "circle_radius = 3 # 20\n",
    "interpolation_factor = 10\n",
    "with_tail = True\n",
    "sample_range_init = range(1,1) # if you set 'sample_range_init' equal to this, it will actually go through all frames of the input video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/']\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190816', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190817', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190818', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190819', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190820', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190821']\n",
      "[8969]\n",
      "20190821_130100924000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_140101017000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_150101104000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_160101150000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_170101273000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_180101329000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_190101427000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_200101460000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_210101613000already processed, moving to next video\n",
      "[8969]\n",
      "20190821_220101656000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190816', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190817', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190818', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190819', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190820']\n",
      "[8969]\n",
      "20190820_132302966000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_142303114000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_152303305000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_162303411000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_172303463000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_182303549000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_192303658000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_202303727000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_212303750000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_222303914000already processed, moving to next video\n",
      "[8969]\n",
      "20190820_232303962000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190816', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190817', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190818', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190819']\n",
      "[8969]\n",
      "20190819_132001056000already processed, moving to next video\n",
      "[8969]\n",
      "20190819_142001179000already processed, moving to next video\n",
      "[8969]\n",
      "20190819_152001210000already processed, moving to next video\n",
      "[8969]\n",
      "20190819_162001303000already processed, moving to next video\n",
      "[8969]\n",
      "20190819_172001408000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190816', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190817', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190818']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8967]\n",
      "20190818_141102437000already processed, moving to next video\n",
      "[8967]\n",
      "20190818_151102522000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190816', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190817']\n",
      "[8949]\n",
      "20190817_140002320000already processed, moving to next video\n",
      "[8949]\n",
      "20190817_150002438000already processed, moving to next video\n",
      "[8949]\n",
      "20190817_160002477000already processed, moving to next video\n",
      "[8949]\n",
      "20190817_170002514000already processed, moving to next video\n",
      "[8949]\n",
      "20190817_180002621000already processed, moving to next video\n",
      "[8949]\n",
      "20190817_190002669000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190816']\n",
      "[8949]\n",
      "20190816_133006090000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_143006213000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_153006257000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_163006334000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_173006392000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_183006487000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_193006576000already processed, moving to next video\n",
      "[8949]\n",
      "20190816_203006686000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190815']\n",
      "[8949]\n",
      "20190815_133505016000already processed, moving to next video\n",
      "[8949]\n",
      "20190815_143505088000already processed, moving to next video\n",
      "[8949]\n",
      "20190815_153505238000already processed, moving to next video\n",
      "[8949]\n",
      "20190815_163505281000already processed, moving to next video\n",
      "[8949]\n",
      "20190815_173505357000already processed, moving to next video\n",
      "[8949]\n",
      "20190815_183505412000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190814']\n",
      "[8962]\n",
      "20190814_140004981000already processed, moving to next video\n",
      "[8962]\n",
      "20190814_150004991000already processed, moving to next video\n",
      "[8962]\n",
      "20190814_160005155000already processed, moving to next video\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8962]\n",
      "20190814_170005190000already processed, moving to next video\n",
      "[8964]\n",
      "20190814_180005328000already processed, moving to next video\n",
      "[8964]\n",
      "20190814_190005383000already processed, moving to next video\n",
      "[8964]\n",
      "20190814_200005502000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813']\n",
      "[8964]\n",
      "20190813_133005916000already processed, moving to next video\n",
      "[8964]\n",
      "20190813_143006059000already processed, moving to next video\n",
      "[8966]\n",
      "20190813_153006101000already processed, moving to next video\n",
      "[8966]\n",
      "20190813_163006177000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813\\\\.pytest_cache']\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813\\\\.pytest_cache\\\\v']\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190813\\\\.pytest_cache\\\\v\\\\cache']\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190812']\n",
      "[8966]\n",
      "20190812_135602574000already processed, moving to next video\n",
      "[8966]\n",
      "20190812_145602638000already processed, moving to next video\n",
      "[8966]\n",
      "20190812_155602640000already processed, moving to next video\n",
      "[8966]\n",
      "20190812_165602754000already processed, moving to next video\n",
      "[8967]\n",
      "20190812_175602930000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190811']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8967]\n",
      "20190811_133005954000already processed, moving to next video\n",
      "[8967]\n",
      "20190811_143006006000already processed, moving to next video\n",
      "[8967]\n",
      "20190811_153006074000already processed, moving to next video\n",
      "[8967]\n",
      "20190811_163006140000already processed, moving to next video\n",
      "[8967]\n",
      "20190811_173006203000already processed, moving to next video\n",
      "[8967]\n",
      "20190811_183006339000already processed, moving to next video\n",
      "[8967]\n",
      "20190811_193006398000already processed, moving to next video\n",
      "[8968]\n",
      "20190811_203006456000already processed, moving to next video\n",
      "[8968]\n",
      "20190811_213006545000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190810']\n",
      "[8969]\n",
      "20190810_133002972000already processed, moving to next video\n",
      "[8969]\n",
      "20190810_143003016000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190809']\n",
      "[8974]\n",
      "20190809_133008080000already processed, moving to next video\n",
      "[8974]\n",
      "20190809_143008134000already processed, moving to next video\n",
      "[8974]\n",
      "20190809_153008196000already processed, moving to next video\n",
      "[8974]\n",
      "20190809_163008266000already processed, moving to next video\n",
      "[8974]\n",
      "20190809_173008372000already processed, moving to next video\n",
      "[8974]\n",
      "20190809_183008413000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190808']\n",
      "[8974]\n",
      "20190808_133005390000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_143005490000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_153005518000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_163005622000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_173005725000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_183005785000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_193005875000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_203005953000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_213006121000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_223006228000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_233006265000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190807']\n",
      "[8974]\n",
      "20190807_140005282000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_150005387000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_160005453000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_170005540000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_180005638000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_190005714000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_200005803000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_210005859000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_220005983000already processed, moving to next video\n",
      "[8974]\n",
      "20190807_230006062000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_000006134000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_010006215000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_020006322000already processed, moving to next video\n",
      "[8974]\n",
      "20190808_030006428000already processed, moving to next video\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190806']\n",
      "[8974]\n",
      "Extracting frames from 20190806_132005490000.mp4\n",
      "17759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started detections extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8879it [2:13:53,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished detections extraction 8033.197139978409\n",
      "detections_to_tracks 20190806_132005490000\n",
      "detections to tracks processed\n",
      "video_export 20190806_132005490000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17758/17758 [35:00<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished video_export 20190806_132005490000\n",
      "[8943]\n",
      "Extracting frames from 20190806_142005603000.mp4\n",
      "17904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started detections extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8952it [2:13:52,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished detections extraction 8032.2100694179535\n",
      "detections_to_tracks 20190806_142005603000\n",
      "detections to tracks processed\n",
      "video_export 20190806_142005603000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17903/17903 [40:57<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished video_export 20190806_142005603000\n",
      "[8961]\n",
      "Extracting frames from 20190806_152005637000.mp4\n",
      "18003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started detections extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [2:14:07,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished detections extraction 8047.133919477463\n",
      "detections_to_tracks 20190806_152005637000\n",
      "detections to tracks processed\n",
      "video_export 20190806_152005637000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18002/18002 [48:21<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished video_export 20190806_152005637000\n",
      "[8919]\n",
      "Extracting frames from 20190806_162005713000.mp4\n",
      "18003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started detections extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [2:13:38,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished detections extraction 8018.669209718704\n",
      "detections_to_tracks 20190806_162005713000\n",
      "detections to tracks processed\n",
      "video_export 20190806_162005713000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18002/18002 [53:14<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished video_export 20190806_162005713000\n",
      "[8837]\n",
      "Extracting frames from 20190806_172005772000.mp4\n",
      "17962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started detections extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8981it [2:17:33,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished detections extraction 8253.354547739029\n",
      "detections_to_tracks 20190806_172005772000\n",
      "detections to tracks processed\n",
      "video_export 20190806_172005772000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17961/17961 [51:41<00:00,  5.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished video_export 20190806_172005772000\n",
      "[8825]\n",
      "Extracting frames from 20190806_182005831000.mp4\n",
      "3662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started detections extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1831it [27:51,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished detections extraction 1671.9396257400513\n",
      "detections_to_tracks 20190806_182005831000\n",
      "detections to tracks processed\n",
      "video_export 20190806_182005831000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3661/3661 [10:10<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished video_export 20190806_182005831000\n",
      "['Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190731', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190801', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190802', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190803', 'Z:/baboon/archive/processed/video/thermal/2019_summer/cliff_data/mp4_files/viewpoint_1/T1020/20190804']\n",
      "[8804]\n",
      "Extracting frames from 20190804_133009112000.mp4\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError -529697949] Windows Error 0xe06d7363",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-077cbda6bf78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                                         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvr\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m]\u001b[0m  \u001b[1;31m# read an image from the capture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decord\\video_reader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek_accurate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decord\\video_reader.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CAPI_VideoReaderNextFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decord\\_ffi\\_ctypes\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mret_tcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         check_call(_LIB.DECORDFuncCall(\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-077cbda6bf78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    157\u001b[0m                                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                                     \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## pull out all the relevant frames as numpy arrays, in a single operation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                                     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# loop through the frames. For each frame...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decord\\video_reader.py\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_nd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CAPI_VideoReaderGetBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbridge_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decord\\_ffi\\_ctypes\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mret_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDECORDValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mret_tcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         check_call(_LIB.DECORDFuncCall(\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError -529697949] Windows Error 0xe06d7363"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if os.path.isdir( metadata.folder_data_server ):\n",
    "\n",
    "    ### we are going to scan through the directory of metadata.folder_data_server, including all of its subdirectories, for videos to process\n",
    "\n",
    "    ## instantiate a list of paths to search through for videos. This will start with just the main directory entered above, and subdirectories will be added upon iterating\n",
    "    paths = [metadata.folder_data_server] \n",
    "\n",
    "    while paths:\n",
    "\n",
    "        print( paths )\n",
    "        # paths.pop() gets top of directory stack to process\n",
    "        # os.scandir is easier and more efficient than os.listdir,\n",
    "        # though it must be closed (but with statement does this for us)\n",
    "        with os.scandir(paths.pop()) as entries:\n",
    "            \n",
    "            for entry in entries:  # loop through the folder\n",
    "\n",
    "                ## set the frame where we want to start the tracking\n",
    "                start = start_init\n",
    "\n",
    "                ## set the frame where we want to end the tracking\n",
    "                end = end_init\n",
    "                \n",
    "                ## set the range of frames that will be used in the video of the tracking below\n",
    "                sample_range = sample_range_init\n",
    "                \n",
    "                #print(file.name)  # print text to keep track the process\n",
    "                if entry.name.endswith( vid_file_ext ):\n",
    "\n",
    "                    ## save the file path of the input video\n",
    "                    input_video_file = entry.path\n",
    "\n",
    "                    ## extract the name of the file from the file path \n",
    "                    vid_name = entry.name.split( '.' )[ 0 ]\n",
    "\n",
    "                    ## extract the piece of the file path that is 'leftover' from the input video folder entered above (without the actual file name). This will be used to mirror the file structure of the input videos for the tracking output videos\n",
    "                    new_dir = entry.path.split( metadata.folder_data_server )[1].split(entry.name)[0]\n",
    "\n",
    "                    ## you can use another csv here with some metadata in it to check whether the video has baboons in it, and only do tracking if there are baboons in it. For now, just set this to be 1 (treat all videos as if there are baboons in them)\n",
    "                    video_has_baboons = 1\n",
    "\n",
    "                    if video_has_baboons:\n",
    "\n",
    "                        ## save the current time\n",
    "                        t = time.time()\n",
    "\n",
    "                        ## empty the GPU cache\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                        get_gpu_memory()\n",
    "\n",
    "                        ##### prepare to set up the output files for the detections, tracks, and video\n",
    "\n",
    "                        ### detections, tracks, and positions will all be added to the output folder (specified above) in a folder structure mirroring the input folder structure; each file will also be labeled with video name that it came from \n",
    "\n",
    "                        # the detections, tracks, and positions, and tracking video will be given the name of the video file (with prefex)\n",
    "                        detections_file_name = \"detections_\" + vid_name + \".npy\" \n",
    "\n",
    "                        ## join the path of the output folder and the detection file name to get the full path to the detection file\n",
    "                        detections_file = os.path.join( metadata.folder_output + new_dir , detections_file_name)\n",
    "\n",
    "                        ## check if this detection file already exists\n",
    "                        detections_exist = os.path.isfile(detections_file)\n",
    "\n",
    "                        ## assign the name of the tracks file\n",
    "                        tracks_file_name = \"tracks_\" + vid_name + \".npy\" \n",
    "\n",
    "                        ## join the path of the output folder and the track file name to get the full path to the track file\n",
    "                        tracks_file = os.path.join(metadata.folder_output + new_dir , tracks_file_name)\n",
    "\n",
    "                        ## check if this track file already exists\n",
    "                        tracks_exist = os.path.isfile(tracks_file)\n",
    "\n",
    "                        video_out_file_name = \"video_\" + vid_name + \".mp4\" \n",
    "                        \n",
    "                        ## assign the name of the video file for the video with the tracks\n",
    "                        video_out_file = os.path.join(metadata.folder_output + new_dir , video_out_file_name)\n",
    "                        \n",
    "                        ## check if the tracking video file already exists\n",
    "                        video_export_exist = os.path.isfile(video_out_file)\n",
    "                        \n",
    "                        if detections_exist and tracks_exist and video_export_exist: ## if this video has already been processed\n",
    "                            \n",
    "                            print( str( vid_name ) + 'already processed, moving to next video')\n",
    "                            ## move to the next video file for tracking\n",
    "                            continue\n",
    "\n",
    "                        ### several lines below are an expansion of this line: video_to_frames(video_path=videopath, frames_dir=(metadata.folder_images), overwrite=True, every=1)\n",
    "\n",
    "\n",
    "                        #### the following lines of code extract all the frames from the input video\n",
    "\n",
    "                        if frames_already_extracted != True:\n",
    "\n",
    "                            ## not really sure what the os.path.join does here, but this just sets up the folder where the images will be extracted to\n",
    "                            os.makedirs( ( os.path.join( metadata.folder_images ) ), exist_ok=True )\n",
    "\n",
    "                            print('Extracting frames from {}'.format(entry.name))\n",
    "\n",
    "                            assert os.path.exists( input_video_file )  # assert the video file exists\n",
    "\n",
    "                            # load the VideoReader\n",
    "                            vr = VideoReader( input_video_file, ctx = cpu(0) )  # can set to cpu or gpu .. ctx=gpu(0)\n",
    "\n",
    "                            if start < 0:  # if start isn't specified lets assume 0\n",
    "                                start = 0\n",
    "                            if end < 0:  # if end isn't specified assume the end of the video\n",
    "                                end = len(vr)\n",
    "\n",
    "                            ## you can change every to be image_interval everywhere below if you want\n",
    "                            every = image_interval\n",
    "\n",
    "                            ## make a list of the frame indices that we will pull out from the video (this is every frame when every = 1 )\n",
    "                            frames_list = list(range(start, end, every ) )\n",
    "\n",
    "                            ## start a counter of the number of images that are saved (why can't this be inferred from the length of frames_list?)\n",
    "                            saved_count = 0\n",
    "\n",
    "                            if every > 50 and len(frames_list) < 1000:  # this is faster for every > 25 frames and can fit in memory\n",
    "\n",
    "                                frames = vr.get_batch(frames_list).asnumpy() ## pull out all the relevant frames as numpy arrays, in a single operation \n",
    "\n",
    "                                for index, frame in zip(frames_list, frames):  # loop through the frames. For each frame...\n",
    "\n",
    "                                    save_path = os.path.join( metadata.folder_images, \"{:06d}.jpg\".format(index))  # create the save path. This will save each frame as a JPG with a 6 digit number as a file name (which indicates which frame in the original video it was) in the image folder input above\n",
    "\n",
    "                                    if not os.path.exists(save_path) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
    "\n",
    "                                        cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # save the extracted image\n",
    "\n",
    "                                        saved_count += 1  # increment our counter by one\n",
    "\n",
    "                            else:  # this is faster for every <25 and consumes small memory\n",
    "\n",
    "                                ## This bit doesn't always work for short videos. So in the last video of a night, it often throws an error. So I will create a try and except statement so that if it throws an error, it uses the above technique, which works well for small videos\n",
    "                                \n",
    "                                try: \n",
    "                                    for index in frames_list:  # lets loop through the frames until the end\n",
    "\n",
    "                                        ## why can't the line above just read: for index in frames_list: ?? (it was for index in range( start, end )  Then we don't have to set the if statement below\n",
    "\n",
    "                                        frame = vr[ index ]  # read an image from the capture\n",
    "\n",
    "                                        # this shouldn't be needad anymore with my adjustment above... if index % every == 0:  # if this is a frame we want to write out based on the 'every' argument\n",
    "\n",
    "                                        save_path = os.path.join( metadata.folder_images, \"{:06d}.jpg\".format(index))  # create the save path. This will save each frame as a JPG with a 6 digit number as a file name (which indicates which frame in the original video it was) in the image folder input above\n",
    "\n",
    "                                        if not os.path.exists( save_path ) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
    "\n",
    "                                            cv2.imwrite(save_path, cv2.cvtColor(frame.asnumpy(), cv2.COLOR_RGB2BGR))  # save the extracted image\n",
    "\n",
    "                                            saved_count += 1  # increment our counter by one\n",
    "\n",
    "                                except:\n",
    "                                    \n",
    "                                    frames = vr.get_batch(frames_list).asnumpy() ## pull out all the relevant frames as numpy arrays, in a single operation \n",
    "\n",
    "                                    for index, frame in zip(frames_list, frames):  # loop through the frames. For each frame...\n",
    "\n",
    "                                        save_path = os.path.join( metadata.folder_images, \"{:06d}.jpg\".format(index))  # create the save path. This will save each frame as a JPG with a 6 digit number as a file name (which indicates which frame in the original video it was) in the image folder input above\n",
    "\n",
    "                                        if not os.path.exists(save_path) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
    "\n",
    "                                            cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # save the extracted image\n",
    "\n",
    "                                            saved_count += 1  # increment our counter by one\n",
    "\n",
    "                            print( saved_count )  # and return the count of the images we saved\n",
    "\n",
    "                        # training\n",
    "\n",
    "                        ###### now run inference on the frames that we have extracted from the video\n",
    "\n",
    "                        if not detections_exist: # if the model hasn't already been used to infer detections from the frames of this video\n",
    "\n",
    "                            # run_inference(vid_name,1) ## this line is expanded out below\n",
    "\n",
    "\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                            from IPython import get_ipython\n",
    "                            import torch, torchvision, detectron2\n",
    "                            from detectron2.utils.logger import setup_logger\n",
    "                            import subprocess as sp, numpy as np, cv2, random, glob, torch, time, pickle, matplotlib.pyplot as plt\n",
    "                            from detectron2 import model_zoo\n",
    "                            from detectron2.engine import DefaultPredictor\n",
    "                            from detectron2.config import get_cfg\n",
    "                            from detectron2.utils.visualizer import Visualizer\n",
    "                            from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "                            from detectron2.modeling import build_model\n",
    "                            from detectron2.checkpoint import DetectionCheckpointer\n",
    "                            from detectron2.data import detection_utils as utils\n",
    "                            from detectron2.data import samplers\n",
    "                            from torch.utils.data import Dataset, DataLoader\n",
    "                            from read_metadata_HIWI import read_metadata\n",
    "                            from tqdm import tqdm\n",
    "                            import subprocess\n",
    "\n",
    "\n",
    "                            class PredictDataset(Dataset):\n",
    "\n",
    "                                def __init__(self, glob_string):\n",
    "                                    self.image_files = sorted(glob.glob(glob_string))\n",
    "\n",
    "                                def __len__(self):\n",
    "                                    return len(self.image_files) // 2\n",
    "\n",
    "                                def __getitem__(self, idx):\n",
    "                                    image_raw = cv2.imread(self.image_files[(idx * 2)])\n",
    "                                    height, width = image_raw.shape[:2]\n",
    "                                    image = torch.as_tensor(image_raw.astype('float32').transpose(2, 0, 1)).contiguous()\n",
    "                                    image_dict0 = {'image':image,  'height':height,  'width':width,  'file_name':self.image_files[idx * 2]}\n",
    "                                    image_raw = cv2.imread(self.image_files[(idx * 2 + 1)])\n",
    "                                    height, width = image_raw.shape[:2]\n",
    "                                    image = torch.as_tensor(image_raw.astype('float32').transpose(2, 0, 1)).contiguous()\n",
    "                                    image_dict1 = {'image':image,  'height':height,  'width':width,  'file_name':self.image_files[idx * 2 + 1]}\n",
    "                                    return [image_dict0, image_dict1]\n",
    "\n",
    "\n",
    "                            original_name = entry.name # pulls the filename of the input video file (from our loop above)\n",
    "\n",
    "                            if not os.path.exists( os.path.join(metadata.folder_output, new_dir ) ): # make the output directory if it doesn't already exist\n",
    "                                os.makedirs( os.path.join(metadata.folder_output, new_dir ) )\n",
    "\n",
    "                            cfg = get_cfg() # obtain detectron2's default configuration\n",
    "\n",
    "                            cfg.merge_from_file(metadata.folder_detectron + 'configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml') # load values for the configuration of the neural network\n",
    "\n",
    "                            cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = SCORE_THRESH_TEST\n",
    "                            cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = BATCH_SIZE_PER_IMAGE\n",
    "                            cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "                            cfg.TEST.DETECTIONS_PER_IMAGE = DETECTIONS_PER_IMAGE\n",
    "                            cfg.SOLVER.BASE_LR = BASE_LR\n",
    "                            cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "                            cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = NMS_THRESH_TEST\n",
    "\n",
    "                            cfg.MODEL.WEIGHTS = metadata.baboon_weights # specifies the model weights from the trained model (specified above)\n",
    "\n",
    "                            dataset = PredictDataset(os.path.join(metadata.folder_images, '*.jpg')) # define the dataset for inference (these are all of the frames that we extracted from the input video above)\n",
    "\n",
    "                            model = build_model( cfg ) # builds the model structure, but only fills it with random parameters\n",
    "\n",
    "                            _ = model.eval() # unclear exactly what this does\n",
    "\n",
    "                            checkpointer = DetectionCheckpointer( model )\n",
    "\n",
    "                            _ = checkpointer.load( cfg.MODEL.WEIGHTS ) # loads the model weights from the trained models onto the model structure that is now build\n",
    "\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                            data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0) # processes the frames into a format needed by the model for inference\n",
    "\n",
    "                            cuda0 = torch.device( 'cuda:0' )\n",
    "\n",
    "                            print('started detections extraction')\n",
    "\n",
    "                            t = time.time()\n",
    "\n",
    "                            with torch.no_grad():\n",
    "\n",
    "                                for batch_num, image_batch in tqdm( enumerate(data_loader) ): # loop through the batches extracted from the input video. (each batch appears to contain 2 frames). For each batch..\n",
    "\n",
    "                                    if batch_num >= max_batches: # break the loop after getting to the max batches (specified above)\n",
    "\n",
    "                                        break\n",
    "\n",
    "                                    for i in range(len(image_batch)): # loop through the frames in the batch. For each frame...\n",
    "\n",
    "                                        ## format the frame for cuda (I am guessing?)\n",
    "                                        image_batch[i]['image'] = np.squeeze(image_batch[i]['image'])\n",
    "                                        image_batch[i]['image'] = image_batch[i]['image'].to(cuda0)\n",
    "                                        image_batch[i]['width'] = image_batch[i]['width'].to(cuda0).item()\n",
    "                                        image_batch[i]['height'] = image_batch[i]['height'].to(cuda0).item()\n",
    "\n",
    "                                    predictions = model( image_batch ) # make predictions on the frames in the batch\n",
    "\n",
    "                                    ## predictions is a list of dictionaries. Each dictionary is associated with one frame from the image batch and contains the coordinates of the corners of the bounding boxes of each detected object in that frame (as well as some metadata)                       \n",
    "\n",
    "                                    for preds, im_dict in zip(predictions, image_batch): # loop through the frames in the batch and their associated predications. For each frame\n",
    "\n",
    "                                        frame_number = os.path.splitext(os.path.basename(im_dict['file_name'][0]))[0] # extracts the base name of the image file of the frame without the file extension. Thus, it extracts just the number associated with the frame (as the names of the image files were just the frame numbers)\n",
    "\n",
    "                                        pkl_file_path = os.path.join(metadata.folder_output, new_dir, '{}-predictions.pkl'.format(frame_number)) # assign the file path of the pkl file (which will the detections) to be produced. This gives the name of the pkl file the same number as the image file of the frame\n",
    "\n",
    "                                        preds_instance = preds['instances'].to('cpu') # extract the detections from the frame\n",
    "\n",
    "                                        ## write a pkl file with the detections from the frame\n",
    "                                        with open(pkl_file_path, 'wb') as (out):\n",
    "                                            pickle.dump(preds_instance, out)\n",
    "                                            out.close()\n",
    "\n",
    "                            print('finished detections extraction ' + str(time.time() - t)) # print the time it took to run the inference and extract the detections\n",
    "\n",
    "\n",
    "                            pkl_files = sorted(glob.glob(os.path.join(metadata.folder_output, new_dir, '*-predictions.pkl'))) # create a list of all the pkl files (there should be one for each frame, containing all the detections for that frame)\n",
    "\n",
    "                            all_detections = [] # instantiate an empty list, which will become a list of dictionaries. Each dictionary will correspond to a frame of the video and will contain the coordinates of the bounding boxes and some metadata (essentially, the same that is included in 'predictions' above, with some reformatting); but this list will, at the end of the loop, contain a dictionary for every frame in the video (or at least all those specified above)\n",
    "\n",
    "                            raw_instances = [] # instantiate an empty list, which will become a list of dictionaries that will represent almost exactly those seen in 'predictions' above, without reformatting (just subsetted to 'instances'); but again, this list will have a dictionary for every frame in the video, by the end of the loop\n",
    "\n",
    "                            for pkl_file in pkl_files[:]: # loop through the pkl files (each associated with one frame of the video). For each pkl file...\n",
    "\n",
    "                                with open(pkl_file, 'rb') as (readfile): # open the pkl file and save the contents (which are predictions[i]['instances'] from above) as 'detections'\n",
    "                                    detections = pickle.load(readfile)\n",
    "\n",
    "                                detection_dict = detections.get_fields() # create a dictionary with the detections from this frame\n",
    "\n",
    "                                ## reformat the detections\n",
    "                                detection_dict['pred_boxes'] = detection_dict['pred_boxes'].tensor.numpy()\n",
    "                                detection_dict['scores'] = detection_dict['scores'].numpy()\n",
    "                                detection_dict['pred_classes'] = detection_dict['pred_classes'].numpy()\n",
    "                                detection_dict['image_name'] = os.path.basename( pkl_file ).split('-')[0]\n",
    "\n",
    "                                all_detections.append(detection_dict) # add the reformatted detections from this frame to the running list of those for all the frames\n",
    "                                raw_instances.append(detections) # add the original detections (not reformatted) from this frame to a running list to those for all the frames\n",
    "\n",
    "                            np.save(detections_file, all_detections) # save all the detections from every frame to the detections_file in the output folder\n",
    "\n",
    "                            fig = plt.figure( figsize=(24, 28) ) # establish a plot size\n",
    "\n",
    "                            detections = np.load( detections_file , allow_pickle=True ) # read the detections file back in (contains all the detections from all frames; this should be the same as 'all_detections' above) \n",
    "\n",
    "                            for image_ind in random.sample( range( 0, len( detections ) ), 5 ): # for 5 random frames in the video (well, of the frames that were predicted on)....\n",
    "\n",
    "                                img = plt.imread( metadata.folder_images + detections[image_ind]['image_name'] + '.jpg' ) # prepare to plot the frame (using the 'image_name' field in 'detections' to find the corresponding frame) \n",
    "\n",
    "                                plt.imshow(img) # plot the frame\n",
    "\n",
    "                                ax = plt.gca() # get current axes\n",
    "\n",
    "                                for item in range(0, len(detections[image_ind]['pred_boxes'])): # loop through the predictions/detections (i.e. bounding boxes) for this frame. For each detection...\n",
    "\n",
    "                                    ## extract the coordinates of the corners of the bounding box\n",
    "                                    x1 = detections[image_ind]['pred_boxes'][item][0]\n",
    "                                    x2 = detections[image_ind]['pred_boxes'][item][2]\n",
    "                                    y1 = detections[image_ind]['pred_boxes'][item][1]\n",
    "                                    y2 = detections[image_ind]['pred_boxes'][item][3]\n",
    "\n",
    "                                    scoretext = str( '{0:.2g}'.format(detections[image_ind]['scores'][item]) ) # extract the score associated with each bounding box and make it a string\n",
    "                                    wid = x2 - x1 # calculate the width of the bounding box\n",
    "                                    hei = y2 - y1 # calculate the height of the bounding box\n",
    "                                    rect = plt.Rectangle((x1, y1), wid, hei, linewidth=1, edgecolor='c', facecolor='none') # prepare plot the bounding box as a rectangle on the frame\n",
    "                                    ax.add_patch(rect) # plot the bounding box\n",
    "                                    ax.annotate(scoretext, (x1, y1), size=20) # add the score associated with the bound box / detection to the plot\n",
    "                                    plt.scatter(x=[x1, x2], y=[y1, y2], c='r', s=10) # plot red dots at the corners of the bounding boxes\n",
    "\n",
    "                                save_dir = os.path.join( metadata.folder_output, new_dir, 'example_detections/' )\n",
    "\n",
    "                                os.makedirs( save_dir, exist_ok = True )\n",
    "\n",
    "                                plt.savefig( os.path.join( save_dir, vid_name + '_' + detections[image_ind]['image_name'] + '.jpg' ), bbox_inches='tight') # save the plot in a folder called 'example detections' which will be in the output folder (under the relevant subdirectory, if applicable)\n",
    "\n",
    "                                plt.clf() # clear the plot\n",
    "\n",
    "                            ## the next few lines just remove all the pkl files. We don't need them anymore now that we have all the detections for all of the frames in a single file\n",
    "                            files_in_directory = os.listdir( os.path.join( metadata.folder_output, new_dir ) ) # make a list of all the files in the directory where we were saving the pkl files\n",
    "\n",
    "                            filtered_files = [ file_2_rem for file_2_rem in files_in_directory if file_2_rem.endswith('.pkl')] # subst this list to the pkl files\n",
    "\n",
    "                            for remove_file in filtered_files: # for each pkl file...\n",
    "                                path_to_file = os.path.join(  metadata.folder_output, new_dir, remove_file ) # save the path to the pkl file\n",
    "                                os.remove( path_to_file ) # remove the pkl file, using the path above\n",
    "\n",
    "                        else: \n",
    "                            \n",
    "                            detections = np.load( detections_file , allow_pickle=True ) # read the detections file back in (contains all the detections from all frames; this should be the same as 'all_detections' above) \n",
    "\n",
    "                        # okay decompiling roi_inference.cpython-36.pyc\n",
    "\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                        ######### the next several lines turn the detections into tracks\n",
    "\n",
    "                        if not tracks_exist: # if the tracks have not yet been extracted and saved (can add an else statement here printing that the tracks already exist)\n",
    "\n",
    "                            print('detections_to_tracks ' + vid_name)\n",
    "\n",
    "                            #### this line is expanded below:  detections_to_tracks( vid_name )\n",
    "\n",
    "                            import numpy as np, matplotlib.pyplot as plt, matplotlib.cm as cm\n",
    "                            from scipy.optimize import linear_sum_assignment\n",
    "                            import os, skimage.transform as transform, cv2, sys\n",
    "\n",
    "                            sys.path.append( os.path.join( sys.path[0], './functions') )\n",
    "\n",
    "\n",
    "                            from koger_tracking import normalize, create_new_track, finalize_track, _get_boxes_center\n",
    "                            from koger_tracking import calculate_distances, calculate_active_list\n",
    "                            from koger_tracking import calculate_max_distance, calculate_active_list\n",
    "\n",
    "                            os.chdir(metadata.folder_code)\n",
    "\n",
    "                            ## check to see if a positions file already exists (positions are just the center points of the bounding boxes saved in detections)\n",
    "                            positions_file = os.path.join( metadata.folder_output, new_dir, 'positions_' + vid_name + '.npy' )\n",
    "                            positions_exists = os.path.isfile( positions_file )\n",
    "\n",
    "                            if positions_exists: # if the positions file already exists\n",
    "\n",
    "                                print('Using already computed postions.  Ignoring box files...')\n",
    "\n",
    "                                positions = np.load( positions_file, allow_pickle=True ) # load the positions file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            for dict_ind, detection_dict in enumerate(detections): # loop through the frames. Just a reminder that detections is a list of dictionaries. Each dictionary corresponds to one frame in the video and contains the detections (bounding boxes) and some metadata for that frame. For each frame...\n",
    "\n",
    "                                scores = detection_dict['scores'] # save the scores associated with each bounding box/detection in this frame\n",
    "\n",
    "                                if positions_exists: # if the already positions exist...\n",
    "                                    \n",
    "                                    positions[dict_ind] = positions[dict_ind][np.where(scores > score_thresh)]  # remove the positions where the score for the detection is below the threshold\n",
    "\n",
    "                                else: # if the predictions don't yet exist...\n",
    "                                    detections[dict_ind]['pred_boxes'] = detection_dict['pred_boxes'][np.where(scores > score_thresh)] # remove the bounding boxes/detections where the score is below the threshold\n",
    "\n",
    "                                detections[dict_ind]['pred_classes'] = detection_dict['pred_classes'][np.where(scores > score_thresh)] # remove the prediction classes associated with detections whose score was below the threshold\n",
    "\n",
    "                                detections[dict_ind]['scores'] = detection_dict['scores'][np.where(scores > score_thresh)] # remove the prediction scores associated with detections whose score was below the threshold\n",
    "\n",
    "\n",
    "                            if not positions_exists: # if positions do not yet exist...\n",
    "\n",
    "                                positions = [] # instantiate an empty list that will become a list of nx2 arrays. Each array will correspond to one frame and contain the xy coordinates of the center of each bounding box on that frame\n",
    "\n",
    "                                for d_dict in detections: # for each dictionary in detections (each frame)...\n",
    "\n",
    "                                    boxes = d_dict['pred_boxes'] # extract all the bounding boxes from the frame\n",
    "\n",
    "                                    positions.append(_get_boxes_center(boxes)) # append an array of the center points of all the bounding boxes to the ongoing positions list \n",
    "\n",
    "                                np.save(positions_file, positions) # save the positions (the list of arrays) to the positions_file\n",
    "\n",
    "                            shape = [] # instantiate an empty list that will tell us how many detections were made in each frame\n",
    "\n",
    "                            for frame_positions in positions: # loop through the list of position arrays (one array for each frame). For each frame...\n",
    "\n",
    "                                shape.append(frame_positions.shape[0]) # find the number of rows in the associated array, and append it to the shape list. This is the number of detections that were made (and that exceeded the threshold score) in that frame\n",
    "\n",
    "                            classes_exists = False # this defines whether there are different classes of objects have been specified in training and inference (but I'm not sure what this would actually do for the tracking). This has to remain as 'False' unless the 'classes_list' is defined somewhere\n",
    "\n",
    "\n",
    "                            raw_track_list = [] # instantiate an empty list that will eventually be filled with all the tracks from the input video\n",
    "\n",
    "                            class_label = None\n",
    "                            head_position = None\n",
    "\n",
    "\n",
    "                            start_frame = 0 # declare the start frame where to start the tracking. Here, we will start at the first frame passed to the model for inference (the first frame of the video, unless specified otherwise above)\n",
    "                            end_frame = len(positions) # declare the end frame where the tracking will end. Here, we will end at the last frame passed to the mode inference (the last frame of the video, unless specified otherwise above)\n",
    "                            #     end_frame = 30000 # use this to cut the tracking off at a given frame\n",
    "\n",
    "                            # Create initial tracks based on the objects in the first frame\n",
    "\n",
    "                            for ind in range(positions[start_frame].shape[0]): # loop through the detections/positions in the starting frame. For each detection....\n",
    "                                #each track entry is both an array that store all points added to the track and a value that\n",
    "                                #indicates the frame on which the track begins\n",
    "\n",
    "                                if classes_exists: ## if different classes have been specified in training and inference\n",
    "                                    class_label = classes_list[start_frame][ind] ## hmmm, there's a problem here, because classes_list doesn't exist...??\n",
    "\n",
    "                                raw_track_list.append(create_new_track(\n",
    "                                    start_frame, positions[start_frame][ind], ind, class_label, head_position, noise=0)) ## this function just puts all of these inputs into a dictionary, that defines the start position and frame of a track. Append this dictionary to the list of dictionaries, with each dictionary representing a track\n",
    "\n",
    "                            ## here's an example of what raw_track_list looks like at this point\n",
    "\n",
    "    #                         [{'track': [array([ 289.91656494, 1074.89868164])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [0],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([2836.65014648, 1517.27270508])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [1],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([ 492.93096924, 1231.10778809])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [2],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([ 753.91522217, 1342.40161133])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [3],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([ 590.48443604, 1262.21020508])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [4],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([2748.73510742, 1369.71362305])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [5],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([1070.95874023, 1524.89660645])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [6],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([ 268.89401245, 1279.27429199])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [7],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([1001.3649292 , 1473.62841797])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [8],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([767.80609131, 879.22998047])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [9],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([ 553.79364014, 1253.65649414])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [10],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0},\n",
    "    #                          {'track': [array([2706.47680664, 1301.63659668])],\n",
    "    #                           'first_frame': 0,\n",
    "    #                           'last_frame': 0,\n",
    "    #                           'pos_index': [11],\n",
    "    #                           'class': [None],\n",
    "    #                           'contour': [None],\n",
    "    #                           'size': [None],\n",
    "    #                           'debug': ['0 created'],\n",
    "    #                           'noise': 0}]\n",
    "\n",
    "                            #try to connect points to the next frame\n",
    "                            for frame in range(start_frame + 1, end_frame): # loop through the rest of the frame indices. For each frame index...\n",
    "                                #if frame % 1000 == 0:\n",
    "                                #    print('frame number', frame, 'processed')\n",
    "\n",
    "                                #get tracks that are still active (have been seen within the specified time)\n",
    "                                active_list = calculate_active_list(raw_track_list, max_unseen_time, frame) # this function scans through all the tracks and inspects whether their time last seen is less than the max_unseen_time threshold. If it is, then it appends the index of this track to the active track list and returns the active track list at the end.\n",
    "                                # the end result of active_list, is therefore a list of integers, indicating the indices of tracks within raw_track_list that are still active\n",
    "\n",
    "                                #positions from the next step\n",
    "                                positions1 = positions[ frame ] # pull out the positions of detections in this frame \n",
    "\n",
    "\n",
    "                                #No existing tracks to connect to\n",
    "                                if len(active_list) == 0: # if there are no existing tracks\n",
    "\n",
    "                                    #Every point in next frame must start a new track\n",
    "                                    if len(positions1) != 0: # if there are detections in this frame\n",
    "\n",
    "                                        for position in range(positions1.shape[0]): # loop through the detections on this frame. For each detection/position...\n",
    "\n",
    "                                            if classes_exists:\n",
    "                                                class_label = classes_list[frame][position]\n",
    "\n",
    "                                            raw_track_list.append(create_new_track(frame, positions1[position], position, class_label, head_position)) # start a new track for each detection in this frame\n",
    "\n",
    "                                    continue # continue to the next frame at this point\n",
    "\n",
    "                                # To keep track of index of new points added for things like \n",
    "                                # Keeping track of image locations, remove indexs that are already added\n",
    "                                positions1_indexes = np.arange(positions1.shape[0]) # save a 1D array of all integers from 0 to the number of detection in this frame\n",
    "\n",
    "                                # This will be changed linking information if points to connect\n",
    "                                # If not, will reamin empty\n",
    "                                row_ind_full = []\n",
    "\n",
    "                                # Make sure there are new points to add\n",
    "                                if len(positions1) != 0: # if there are detections in this frame...\n",
    "\n",
    "    #                                 this line is expanded below: raw_track_list = calculate_max_distance(raw_track_list, \n",
    "    #                                                                         active_list, \n",
    "    #                                                                         max_distance_threshold, \n",
    "    #                                                                         max_distance_threshold_noise, \n",
    "    #                                                                         min_distance_threshold) # this function updates the 'max_distance' value in the raw_track_list dictionary. To be honest, I don't fully understand the max_distance value\n",
    "\n",
    "\n",
    "                                    ## the next several lines find the nearest neighbor distances for each of the currently active tracks (using their latest positions). We are trying to find the distance that each track can 'look' for a new detection to add to its track. \n",
    "                                    ## This distance is determined by a max distance threshold we set, as well as a the location of the other tracks. A track can't look beyond it's nearest neighbor track for its next detection (actually, it can't look past about the midway -- actually 45% of the way -- between the track and it's nearest neighbor track, is then the point might belong to the other track)\n",
    "\n",
    "                                    # only check distances to established tracks\n",
    "                                    positions0 = [raw_track_list[active_list[0]]['track'][-1]] # this returns the latest position of the first active track, and makes it the first entry of a list\n",
    "\n",
    "                                    if len(active_list) > 1: # if there is more than one active track...\n",
    "\n",
    "                                        for track_num in active_list[1:]: # loop through the indices of the tracks that are active (i.e. seen within the threshold amount of time). For each active track...\n",
    "\n",
    "                                            if raw_track_list[track_num]['noise'] <= 0: # if the noise parameter in this track is less than or equal to 0.... \n",
    "\n",
    "                                                positions0.append(raw_track_list[track_num]['track'][-1]) # append the latest position in this active track to the ongoing list of latest positions of active tracks\n",
    "\n",
    "\n",
    "                                    positions0 = np.stack(positions0) # turns the list of 1x2 arrays (xy coordinates for the latest position of each active track) into an nx2 array (n = the number of active tracks in which the noise parameter is not > 0)\n",
    "\n",
    "                                    ## this line expanded out below: distance = calculate_distances(positions0, track_list, active_list)\n",
    "\n",
    "                                    old_positions = [raw_track_list[track_num]['track'][-1] for track_num in active_list] # extract the last coordinate of the all the active tracks. This will be the same as positions0 unless some of the active tracks had a noise parameter greater than 0 (in which case they will be left out of positions0 but included in old_positions)\n",
    "                                    old_positions = np.stack(old_positions) # turns the list of 1x2 arrays (xy coordinates for the latest position of each active track) into an mx2 array ( m = the number of active tracks)\n",
    "\n",
    "\n",
    "                                    x_diff = (np.expand_dims(positions0[:, 1], 0) \n",
    "                                              - np.expand_dims(old_positions[:, 1], 1)\n",
    "                                             ) # find the difference in the x-coordinates between all the points (pairwise)\n",
    "\n",
    "                                    y_diff = (np.expand_dims(positions0[:, 0], 0) \n",
    "                                              - np.expand_dims(old_positions[:, 0], 1)\n",
    "                                             ) # find the difference in the y-coordinates between all the points (pairwise)\n",
    "\n",
    "                                    distance = np.sqrt(x_diff ** 2 + y_diff ** 2) # returns an m x n array (with m = the number of active tracks, and n = the number of active tracks with noise parameter not greater than 0), and contains the pairwise distances between the latest points of all active tracks (except for those that exceeded the noise parameter -- see above)\n",
    "\n",
    "                                    # closest point will be itself, so make zero distance \n",
    "                                    # bigger than other distances\n",
    "                                    distance[np.where(distance == 0)] = float(\"inf\") # sets the entries on the diagonal of the array equal to infinity, instead of zero\n",
    "\n",
    "                                    # HYPER PARAMETER\n",
    "                                    # divide by three because saying that within a \n",
    "                                    # third of distance to neighbor much more likely to be\n",
    "                                    # you then them\n",
    "                                    closest_neighbor = np.min(distance, 1) * .45 # finds the nearest neighbor distance of each active track and multiplies them by the hyperparameter\n",
    "\n",
    "                                    # Even if neighbors are all far away, have a max threshold to look for new points \n",
    "                                    closest_neighbor[np.where(closest_neighbor > max_distance_threshold)] = max_distance_threshold # set the nearest neighbor distance equal to the max_distance threshold for the active tracks whose nearest neighbor is actually farther away than this threshold \n",
    "                                    closest_neighbor[np.where(closest_neighbor < min_distance_threshold)] = min_distance_threshold # set the nearest neighbor distance equal to the min_distance threshold for the active tracks whose nearest neighbor is actually closer than this threshold \n",
    "\n",
    "                                    for active_ind, track_num in enumerate(active_list): # loop through the indices of the tracks that are still active. For each active track...\n",
    "\n",
    "                                        raw_track_list[track_num]['max_distance'] = closest_neighbor[active_ind]  # set the max_distance field in the active track's dictionary equal to the nearest neighbor distance (multiplied by the multiplier above; currently 0.45)\n",
    "\n",
    "                                        if raw_track_list[track_num]['noise'] > 0: # if the noise field of this active track is greater than 0 (i.e. this track has remained unseen more than it has remained seen since it was first detected)... \n",
    "\n",
    "                                            if closest_neighbor[active_ind] > max_distance_threshold_noise: # if the closest neighbor for this active track is greater than the max_distance_noise_threshold...\n",
    "\n",
    "                                                raw_track_list[track_num]['max_distance'] = max_distance_threshold_noise # set the max_distance field of this track's dictionary equal to the max_distance_noise_threshold\n",
    "\n",
    "\n",
    "                                        ## HYPER PARAMETER\n",
    "                                        use_size = False\n",
    "\n",
    "                                        ## use_size has been set to False, so I am skipping over this for now (I think it has to do with using the size of the detection (bounding box) to adjust the radius at which we look around for other points to add to the track). Go to Koger functions, calculate_max_distances to find out more...\n",
    "\n",
    "                                        if use_size:\n",
    "                                            size = raw_track_list[track_num]['size'][-1]\n",
    "                                            max_distance = raw_track_list[track_num]['max_distance']\n",
    "                                            if size < 30:\n",
    "                                                max_distance = np.min([15, max_distance]) \n",
    "                                            elif size < 120:\n",
    "                                                max_distance = np.min([20, max_distance])\n",
    "                                            elif not np.isnan(size):\n",
    "                                                if min_distance_big:\n",
    "                                                    # Even is points near by, give room to look around\n",
    "                                                    max_distance = np.max([min_distance_big, max_distance])\n",
    "\n",
    "                                            raw_track_list[track_num]['max_distance'] = max_distance\n",
    "\n",
    "\n",
    "                                    #returns an array of shape (len(active_list), positions1.shape[0])\n",
    "                                    #row is distance from every new point to last point in row's active list \n",
    "                                    ## this line is expanded out below: distance_raw = calculate_distances(positions1, raw_track_list, active_list)\n",
    "                        #             if frame > 4000:\n",
    "\n",
    "\n",
    "                                    #positions from last step\n",
    "                                    old_positions = [raw_track_list[track_num]['track'][-1] for track_num in active_list] # extract the last coordinate of the all the active tracks. This will be the same as positions0 unless some of the active tracks had a noise parameter greater than 0 (in which case they will be left out of positions0 but included in old_positions)\n",
    "                                    old_positions = np.stack(old_positions) # turns the list of 1x2 arrays (xy coordinates for the latest position of each active track) into an mx2 array ( m = the number of active tracks)\n",
    "\n",
    "                                    x_diff = (np.expand_dims(positions1[:, 1], 0) \n",
    "                                              - np.expand_dims(old_positions[:, 1], 1)\n",
    "                                             ) # find the difference in the x-coordinates between all current detections and the latest positions of each active track (pairwise)\n",
    "\n",
    "                                    y_diff = (np.expand_dims(positions1[:, 0], 0) \n",
    "                                              - np.expand_dims(old_positions[:, 0], 1)\n",
    "                                             ) # find the difference in the y-coordinates between all current detections and the latest positions of each active track (pairwise)\n",
    "\n",
    "                                    distance_raw = np.sqrt(x_diff ** 2 + y_diff ** 2) # calculate the distance between all current detections and the latest positions of each active track (pairwise). Store this as a numpy mxn array (m = the number of active tracks, n = the number of detections on the current frame)\n",
    "\n",
    "                                    max_distance = np.zeros_like(distance_raw) # makes an array of 0's with the same dimensions as distance_raw (mxn -- m = the number of active tracks, n = the number of detections on the current frame)\n",
    "                                    not_noise_tracks = np.ones(len(active_list)) # makes a 1D array of ones that is the length of the number of active tracks\n",
    "                                    track_length = np.zeros(len(active_list)) # makes a 1D array of zeros that is the length of the number of active tracks\n",
    "\n",
    "                                    for active_num, track_num in enumerate(active_list): # loop through the indices of the tracks that are active. For each active track...\n",
    "\n",
    "                                        max_distance[active_num, :] = raw_track_list[track_num]['max_distance'] # fill the row of the max_distance array corresponding to this active track with the max_distance to look for a detection\n",
    "\n",
    "                                        track_length[ active_num ] = frame - raw_track_list[track_num]['first_frame'] # calculate the length of the track and insert it in the position of the 1D array corresponding to this active track\n",
    "\n",
    "                                        if raw_track_list[track_num]['noise'] > 0: # if this active track has a noise field greater than 0...\n",
    "\n",
    "                                            not_noise_tracks[active_num] = 0 # switch the 1 to a 0 in the 1D array in the position correspondingto this active track\n",
    "\n",
    "                                    is_max_distance = distance_raw > max_distance ## create an mxn array of booleans declaring whether the distance from each active track to each current detection is too far to consider each current detection a part of each active track\n",
    "\n",
    "                                    new_track = np.all(is_max_distance, 0) # returns 1D array with length equal to the number of new detections. This declares whether each new detection represents a new track (i.e. it is not within range of current active tracks). False implies that it belongs to an active track\n",
    "\n",
    "                                    new_track_ind = np.where(new_track)[0] # returns an array of the indices of current detections that represent new tracks\n",
    "\n",
    "                                    if np.any(new_track): # if any of the current detections represent new tracks (i.e. there are current detections that too far away from active tracks to belong to them)...\n",
    "\n",
    "                                        for t in new_track_ind: # loop through the indices of the current detections that represent new tracks. For each detection that represents a new track...\n",
    "                                            if classes_exists:\n",
    "                                                class_label = classes_list[frame][t]\n",
    "\n",
    "                                            raw_track_list.append(create_new_track(frame, positions1[t], t, class_label, head_position)) # append the position as the start of a new track in the track list\n",
    "\n",
    "                                    # Get rid of new points that are too far away and were just added as new tracks\n",
    "                                    distance = np.delete(distance_raw, new_track_ind, 1) # removes columns from the pairwise distance matrix associated with positions that were just made into new tracks because they were too far away from existing active tracks  \n",
    "\n",
    "                                    if distance.shape[1] > 0: ## if there are current detections that can be assigned to existing tracks...\n",
    "\n",
    "                                        # To keep track of index of new points added for things like \n",
    "                                        # Keeping track of image locations, remove indexs that are already added\n",
    "                                        positions1_indexes = np.delete(positions1_indexes, new_track_ind) # remove the indices of current detections/positions that represented new tracks\n",
    "                                        # Remove the new positions corresponding to added tracks\n",
    "                                        positions1 = np.delete(positions1, new_track_ind, 0) # remove the current detections/positions that represented new tracks\n",
    "\n",
    "                                        #connect the dots from one frame to the next\n",
    "                                        row_ind, col_ind = linear_sum_assignment(distance) # find the optimal distribution of each current detection to an active track, such that it decreases the total distance between current detections and the active tracks that they are added to. Save the indices that tell which current detection matches with which active track\n",
    "                                        # row_ind is the index of the active track that pairs the corresponding index of the current detection indicated in col_ind\n",
    "                                        # i.e. row_ind = [ 1, 3, 5 ], col_ind = [ 0, 2, 1 ] means that the first current detection pairs with the second active track, the second current detection pairs with the sixth active track, and the third current detection pairs with the fourth active track\n",
    "\n",
    "                                        row_ind_full = np.arange(len(active_list)) # make a 1D array that goes from 0 to the length of active_list\n",
    "                                        col_ind_full = np.empty(len(active_list)) # make a 1D array of zeros that is the length of active_list\n",
    "                                        col_ind_full[ : ] = np.nan  ## this switches the empty array to being filled with nan's -- well, actually -2... but it is important that it is not zeroes. See comment below above 'for duplicate in duplicates' for clarification why\n",
    "                                        \n",
    "                                        # In casese where there are fewer new points than existing tracks\n",
    "                                        # some tracks won't get new point. The next several lines will determine which tracks shouldn't get a new point because there is no detection close enough, and which tracks should/shouldn't get a new point when there are two or more tracks competing over the same point\n",
    "                                        # so, the next several lines essentially 'edit' the linear sum assignment performed above\n",
    "\n",
    "                                        duplicates = [] # instantiate an empty list that will be filled with current detections that need to be duplicated because they are becoming a part of more than one track\n",
    "\n",
    "                                        to_delete = [] # instantiate an empty list that will be filled with less competitive track competing for the same current detection as a more competitive track, or tracks that have no current detections close to them\n",
    "\n",
    "                                        for r_ind in row_ind_full: # loop through the indices of the active tracks. For each active track...\n",
    "\n",
    "                                            if r_ind in row_ind: # if this active track has been paired to a new detection by the linear sum assignment...\n",
    "\n",
    "                                                col_ind_full[ r_ind ] = col_ind[ np.where( row_ind == r_ind ) ] # save the index of the current detection that is associated with this active track in the col_ind_full array (this array indicates which current detection each active track pairs with, with 0's representing tracks that don't pair with any current detection). \n",
    "                                                # How do we differentiate between 0 meaning it doesn't associate with a detection, and 0 meaning it is associated with the first detection????? Oh, I think they get deleted below\n",
    "\n",
    "\n",
    "                                            else: # if this active track has NOT been paired to a new detection by the linear sum assignment...\n",
    "\n",
    "                                                if np.min(distance[r_ind]) < raw_track_list[active_list[r_ind]]['max_distance']: # if the closest current detection to this active track is less than the max_distance for this active track (and it just hasn't been paired because it wasn't the least-cost solution determined by linear sum assignment)...\n",
    "\n",
    "                                                    # There is a new point within this tracks assignment range\n",
    "                                                    duplicates.append(np.argmin(distance[r_ind])) # append the index of the current detection that is the closest to this active track to the duplicates list\n",
    "\n",
    "                                                    col_ind_full[r_ind] = duplicates[-1] # save the index of the current detection that is associated with this active track in the col_ind_full array\n",
    "\n",
    "                                                else:\n",
    "\n",
    "                                                    # This track wasn't assigned a new point and there isn't one close by\n",
    "                                                    to_delete.append( r_ind ) # append this index of the active track to the list that will not receive an updated position/detection this frame\n",
    "\n",
    "\n",
    "                                        ## one minor issue can arise in the below loop. If the 0 index (i.e. the first current detection) is in duplicates, the loop below will just attribute the first current detection to either the track it is supposed to be paired with via the linear sum assignment, one of the duplicates, or just a track that hasn't yet been paired with a current detection; whichever one is longest\n",
    "                                        #I fixed this by changing col_ind_full = np.zeros(...) above to col_ind_full = np.empty(...)\n",
    "\n",
    "                                        for duplicate in duplicates: # loop through the indices of the current detections that could be used in more than one track. For each of these detections...\n",
    "\n",
    "                                            competing_tracks = np.squeeze(np.argwhere( col_ind_full == duplicate ) ) # create a 1D array of the indices of the active tracks that this detection could become a part of\n",
    "\n",
    "                                            dominant_track_ind = np.argmax( track_length[ competing_tracks ] ) # find which of these active tracks is the longest (i.e. started the most frames ago / on the earliest frame)\n",
    "\n",
    "                                            to_delete.extend( competing_tracks[ :dominant_track_ind ] ) # add the indices of the competing tracks prior to the index of the dominant track index to the list of tracks that will not receive an updated position/detection this frame \n",
    "\n",
    "                                            if dominant_track_ind  < len(competing_tracks): # if the longest track is not the last track in the list of competing tracks...\n",
    "\n",
    "                                                to_delete.extend(competing_tracks[dominant_track_ind+1:]) # add the indices of the competing tracks beyond to the index of the longest track to the list of tracks that will not receive an updated position/detection this frame \n",
    "\n",
    "                                        to_delete_a  = np.array(to_delete) # turn the list of tracks to delete into an array\n",
    "\n",
    "                                        if not len( to_delete_a ) == 0:\n",
    "\n",
    "                                            #print( col_ind_full )\n",
    "                                            \n",
    "                                            ## for the tracks that won't receive a current detection from this frame (because the current detections are too far away or because they were outcompeted by longer running tracks), remove their indices from the pairing of the tracks to the current detections\n",
    "                                            col_ind_full = np.delete( col_ind_full, to_delete_a ) \n",
    "                                            row_ind_full = np.delete( row_ind_full, to_delete_a )\n",
    "\n",
    "                                            #print( col_ind_full )\n",
    "\n",
    "                                        #see if points got assigned to tracks that are farther than max_threshold_distance\n",
    "                                        #This happens when the closer track gets assigned to a different point\n",
    "\n",
    "                                        col_ind_full = col_ind_full.astype( int ) ## change the float type array to an array of integers (because we are going to index using this array)\n",
    "                                        \n",
    "                                        bad_assign = distance[ row_ind_full, col_ind_full ] > max_distance[:, 0][ row_ind_full ] # save a 1D array of booleans stating whether each pairing of active track to current detection exceeds the maximum distance allowed (based on the threshold and the location of other active tracks)\n",
    "\n",
    "                                        not_noise_tracks_used = not_noise_tracks[ row_ind_full ] # declare whether each active track to which a current detection will be appended is noise or not\n",
    "\n",
    "                                        if np.any(bad_assign): # if there any instances in which the distance between the active track and the current detection it is paired with exceed the allowable maximum distance...\n",
    "\n",
    "                                            bad_assign_points = np.where( bad_assign * not_noise_tracks_used ) # save the indices where the maximum allowable distance is exceeded, and the track was not a noise track\n",
    "\n",
    "                                            #Assign multiple tracks to nearby points, in cases where track got assigned to somewhere far away\n",
    "                                            #because closer track got assigned to point first\n",
    "                                            #this case could come up when two animals get too close so they merge to one point\n",
    "                                            col_ind_full[ bad_assign_points ] = np.argmin( distance[ row_ind_full[ bad_assign_points ], : ], 1) # for the active track that was assigned a point far away, assign to the active track the current detection is closest to it\n",
    "\n",
    "                                            # There may be some tracks that just don't have any new points nearby.  Now filter those out\n",
    "                                            valid_assign = distance[row_ind_full, col_ind_full] <= max_distance[:, 0][row_ind_full] # save a 1D array of booleans declaring whether the distance between the active tracks and the current detections to pair with them are less than or equal to the allowable maximum distance (i.e. did we solve the problem with the previous line? If not, we will not add any current detections to these tracks)\n",
    "\n",
    "                                            ## only keep the pairings between active tracks and current detections that are allowable given the maximum allowable distances\n",
    "                                            col_ind_full = col_ind_full[ valid_assign ]\n",
    "                                            row_ind_full = row_ind_full[ valid_assign ]\n",
    "\n",
    "\n",
    "                                active_list = np.array( active_list ) # turn the active list (the list of indices of the raw tracks that are active) into an array\n",
    "\n",
    "\n",
    "                                for track_num in range( len( raw_track_list ) ): # loop through the indices of the tracks. For each track...\n",
    "\n",
    "                                    if track_num in active_list: # if this track is an active track (i.e. it has been seen since the max_unseen_time)...\n",
    "\n",
    "                                        #Case where there are new points in the next frame to add\n",
    "                                        if track_num in active_list[ row_ind_full ] and len( positions1 ) != 0: # if there are detections in the current frame, and one of these detections has been paired with this track...\n",
    "\n",
    "                                            row_count = np.where( track_num == active_list[row_ind_full] )[0] # locate this active track in row_ind_full, as this index will help us find the current detection that this active track pairs with \n",
    "\n",
    "                                            # If this is a refound track, linearly interpolate from when last seen\n",
    "                                            if raw_track_list[track_num]['last_frame'] != frame - 1: # if the last frame on which this track was seen was not the previous frame...\n",
    "                                                missed_steps = frame - raw_track_list[track_num]['last_frame'] - 1 # calculate how many frames it has been since this track was last seen\n",
    "\n",
    "                                                gap_distance = ( positions1[col_ind_full[row_count[0]]] - raw_track_list[track_num]['track'][-1] ) # find the distance between the current detection to add to this track and the last detection from this track\n",
    "\n",
    "                                                step_distance = gap_distance / (missed_steps + 1) # divide the total distance between the current detection and the last location of this track by the number of frames since this track has been seen to find out how much the track has moved per frame since it was last seen (i.e. linearly interpolate)\n",
    "\n",
    "                                                for step in range(missed_steps): # loop through the frames on which this track hadn't been seen. For each frame...\n",
    "\n",
    "                                                    raw_track_list[track_num]['track'][-step - 1] = (\n",
    "                                                        raw_track_list[track_num]['track'][-step - 1] + \n",
    "                                                        (missed_steps - step) * step_distance) # adjust the track's location on that frame based on the linear interpolation (i.e. to the previous location, add the interpolation step length multiplied by the number of steps)\n",
    "\n",
    "                                            raw_track_list[track_num]['track'].append(positions1[col_ind_full[row_count[0]]]) # add the current detection that is paired with this active track to the track\n",
    "\n",
    "                                            raw_track_list[track_num]['pos_index'].append(positions1_indexes[col_ind_full[row_count[0]]]) # add the index of the current detection that is paired with this active track to the track metadata (I don't fully understand the importance of this)\n",
    "\n",
    "                                            raw_track_list[track_num]['last_frame'] = frame # update the last frame that this active track was seen on with the current frame\n",
    "\n",
    "                                            if raw_track_list[track_num]['noise'] > 0: # if the track's noise is greater than 0 (i.e. it hasn't yet been confirmed as a true track)....\n",
    "                                                raw_track_list[track_num]['noise'] -= 1 #  subtract one from the noise component (i.e. give it a point towards being a real track because it came up again)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                        else: # if the track is in the active list, but there is no current detection associated with it\n",
    "\n",
    "                                            raw_track_list[track_num]['track'].append(raw_track_list[track_num]['track'][-1]) # duplicate the previous location of the track (i.e. act as though the track has not moved from the last frame to this frame)\n",
    "\n",
    "                                            raw_track_list[track_num]['pos_index'].append(np.nan) # add a NaN to the pos_index field of the track metadata (because no current detection as actually added to the track)\n",
    "                                            # isn't a confirmed real track yet\n",
    "                                            if raw_track_list[track_num]['noise'] > 0: # if the noise of the track is greater than 0 (i.e. it is not a confirmed track yet)...\n",
    "                                                raw_track_list[track_num]['noise'] += 1 # add one to the noise component of the track (i.e. dock it a point away from being a real track, because we didn't see it in this frame)\n",
    "\n",
    "                                # Add new tracks for new points that weren't added to existing tracks but weren't far enough away before to aleady get a new track\n",
    "                                if \"distance\" in locals():\n",
    "                                    if distance.shape[0] < distance.shape[1]: # if there are more current detections than active tracks...\n",
    "\n",
    "                                        # There are possible new points\n",
    "                                        for pos1_ind in range(positions1.shape[0]): # loop through the current detections. For each detection...\n",
    "\n",
    "                                            if pos1_ind in col_ind_full: # if this detection is in the list of indices that have already been associated with an existing track...\n",
    "                                                # This point was already added to an existing track\n",
    "                                                continue # move onto the next detection\n",
    "\n",
    "                                            # Only add points that aren't too close to existing tracks \n",
    "                                            if np.min(distance_raw[:, positions1_indexes[pos1_ind]]) > min_new_track_distance: # if the distance from this detection to the closest active track is not below the threshold distance to be considered a new track...\n",
    "                                                # This new point isn't too close to existing tracks\n",
    "\n",
    "                                                if classes_exists:\n",
    "\n",
    "                                                    class_label = classes_list[frame][positions1_indexes[pos1_ind]]\n",
    "\n",
    "                                                raw_track_list.append(create_new_track(\n",
    "                                                    frame, positions1[pos1_ind], positions1_indexes[pos1_ind], class_label, head_position)) # make this detection into the start of a new track\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                # Traverse the list in reverse order so if there are multiple tracks that\n",
    "                                # need to be removed the indexing doesn't get messed up \n",
    "                                for track_num in range(len(raw_track_list) - 1, -1, -1): # loop through the tracks. For each track...\n",
    "\n",
    "                                    if raw_track_list[track_num]['noise'] >= noise_thresh: # if the track's noise score has exceeded 10 #HYPER PARAMETER# (i.e. the net refinding of this track has dipped below -10)...\n",
    "\n",
    "                                        del raw_track_list[track_num] # then remove the track from the track list (i.e. delete it for good)\n",
    "\n",
    "\n",
    "\n",
    "                            # after all the frames have been looped through and all tracks have been formed...\n",
    "\n",
    "                            for track_ind, track in enumerate(raw_track_list): # loop through the tracks. For each track...\n",
    "                                raw_track_list[track_ind] = finalize_track(track) # turn the list of 1x2 coordinates in the track into a nx2 numpy array, where n = the number of frames that the track persisted\n",
    "\n",
    "\n",
    "\n",
    "                            for track in raw_track_list:\n",
    "                                #THIS IS THE PART THAT GETS RID OF THE EXTRA PART OF THE TRAIL\n",
    "                                #number of extra points at the end of track that were added hoping that the point would reapear \n",
    "                                #nearby.  Since the tracking is now finished.  We can now get rid of these extra points tacked on to the end\n",
    "\n",
    "                                track['track'] = track['track'][:track['last_frame'] - track['first_frame'] + 1] # only keep the rows of the nx2 array of coordinates up to the point where the track was actually last seen\n",
    "\n",
    "                                track['pos_index'] = track['pos_index'][:track['last_frame'] - track['first_frame'] + 1] # only keep the indices of the detections up to the point where the track was actually last seen\n",
    "\n",
    "\n",
    "\n",
    "                            print('detections to tracks processed')\n",
    "\n",
    "                            ## filter out the tracks that are shorter than the required number of frames\n",
    "                            track_lengths = [] # instantiate an empty list that will be filled with the length of each track (in terms of number of frames it persisted)\n",
    "                            track_list = [] # instantiate an empty list that will be filled with the tracks to keep\n",
    "\n",
    "                            for track_num, track in enumerate(raw_track_list): # loop through the tracks in the raw track list. For each track...\n",
    "\n",
    "                                track_length = track['track'].shape[0] # save the length of the track\n",
    "\n",
    "                                if track_length >= min_length_threshold: # if the length of the track is greater than the minimum track length threshold\n",
    "\n",
    "                                    track_lengths.append(track['track'].shape[0]) # append the length of the track to the track_lengths list\n",
    "\n",
    "                                    track_list.append(track) # append the track to the final list of tracks\n",
    "\n",
    "                            ## HYPER PARAMTER\n",
    "                            save = True\n",
    "                            if save:\n",
    "\n",
    "                                np.save(tracks_file, track_list) # save the track list to the tracks_file, if save = True\n",
    "\n",
    "                        # for debugging - detections_to_tracks('GH010368')\n",
    "\n",
    "\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                        if not video_export_exist: # if the video file does not already exists\n",
    "\n",
    "                            video_out_file_size = 0 # set the file size equal to zero\n",
    "\n",
    "                        else: # if the video file already exists...\n",
    "\n",
    "                            video_out_file_size = os.path.getsize( video_out_file ) # save the size of the file\n",
    "\n",
    "                        if video_out_file_size < 1000: # if the video file is non-existant or small (i.e. not yet a real video)...\n",
    "\n",
    "                            print('video_export ' + vid_name)\n",
    "\n",
    "                            ## this line is expanded below: export_tracks(vid_name) #  sample_range = range(6000,6300,1)  \n",
    "\n",
    "                            from IPython import get_ipython\n",
    "                            import pandas as pd, numpy as np, cv2, os, glob, matplotlib.pyplot as plt\n",
    "                            from matplotlib import cm\n",
    "                            from read_metadata_HIWI import read_metadata\n",
    "                            from tqdm import tqdm\n",
    "                            from scipy import interpolate\n",
    "                            from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "                            os.path.join(metadata.folder_images, '*.jpg') # I don't think this does anything\n",
    "\n",
    "                            frame_files = sorted( glob.glob( os.path.join( metadata.folder_images, '*.jpg' ) ) ) # save the names of the frame files that were extracted from the input video above\n",
    "\n",
    "                            tracks = np.load( tracks_file, allow_pickle=True ) # load the tracks that we just saved above\n",
    "\n",
    "                            im = cv2.imread(frame_files[0]) # read in the first frame of the video\n",
    "\n",
    "                            fourcc = (cv2.VideoWriter_fourcc)(*'mp4v') # declare the codec for the video\n",
    "\n",
    "                            video_writer = cv2.VideoWriter( video_out_file, fourcc, 23.976, (im.shape[1], im.shape[0]) ) # instantiate the video at the desired file location with the codec declared above. Pull the dimensions for the video from the first frame extracted from the input video\n",
    "\n",
    "                            colors = np.random.uniform(size=(len(tracks))) # generate random numbers between 0 and 1 (with uniform probability). There will be as many numbers as there are tracks\n",
    "\n",
    "                            colors = [get_color(c) for c in colors] # turn each number into a color\n",
    "\n",
    "                            all_tracking = pd.DataFrame({'frame':[],  'id':[],  'x':[],  'y':[]}) # start a dataframe that will be filled with all the location of all the tracks for the video\n",
    "\n",
    "                            all_distance = [] # len(frame_files) # instantiate an empty list\n",
    "\n",
    "                            ## turn the numpy tracks file into a csv\n",
    "                            for track_ind, track in enumerate(tracks): # loop through the tracks. For each track...\n",
    "    \n",
    "                                track_frames = list( range( track['first_frame'], track['last_frame'] + 1 ) ) # make a list of the frames for which this track persists\n",
    "\n",
    "                                to_append = pd.DataFrame( {'frame': pd.Series( track_frames ), 'id': [ track_ind ] * len( track_frames), 'x': pd.Series( track['track'][:,0] ) ,  'y':track['track'][:,1] } ) # make a dataframe of the frame number, the track id, and the xy position of that track id on that frame number \n",
    "\n",
    "                                all_tracking = all_tracking.append( to_append, ignore_index = True ) # add the dataframe for this track to the running dataframe containing all tracks\n",
    "\n",
    "                            all_tracking[ 'vid_name' ] =  vid_name # add the name of the video as a column (so that we can distinguish between track 0 in on hour and track 0 in )\n",
    "\n",
    "                            all_tracking = all_tracking.astype( { 'frame': int, 'id': int, 'x': float, 'y': float } )\n",
    "\n",
    "                            all_tracking.to_csv( os.path.join(metadata.folder_output, new_dir, 'tracks_' + vid_name + '.csv'), index = False) # write the csv with all the tracks to a file\n",
    "\n",
    "                            if len(sample_range) == 0: # if the sample_range has been set to the default (i.e. essentially hasn't been defined)\n",
    "\n",
    "                                sample_range = range(1, len(frame_files), 1) # set the range equal to the entire input video\n",
    "\n",
    "                            for frame_ind in tqdm(sample_range): # loop through the frames of the input video. For each frame...\n",
    "\n",
    "                                im = cv2.imread(frame_files[frame_ind]) # read the frame in using cv2\n",
    "\n",
    "                                for track_ind, track in enumerate(tracks): # loop through the tracks. For each track...\n",
    "\n",
    "                                    rel_frame = frame_ind - track['first_frame'] # find the number of this frame compared to the frame on which the track starts from\n",
    "\n",
    "                                    if rel_frame >= 0 and track['last_frame'] > frame_ind: # if the current frame comes after the track has started and before the track has ended...\n",
    "\n",
    "                                        center_pos = track['track'][rel_frame] # find and save the position of the track on this frame\n",
    "\n",
    "                                        if not np.isnan( np.sum( center_pos ) ):                        \n",
    "\n",
    "                                            cv2.circle(im, (int(center_pos[0]), int(center_pos[1])), circle_radius, colors[track_ind], -1) # print a circle on the frame representing the center position of the current detection (i.e. the location of the track at this frame)\n",
    "                                            cv2.putText(im, str(track_ind), (int(center_pos[0]), int(center_pos[1])), 0, 1, colors[track_ind],3) # label the location of the current detection of the track with the index of the track\n",
    "\n",
    "                                            if with_tail: # if there should be a tail printed on the frame showing where the track had been in previous frames...\n",
    "\n",
    "                                                if rel_frame > steps: # if the number of frames from the start of the track to the current frame are greater than the number of steps to include in the tail...\n",
    "\n",
    "                                                    pos = track['track'][ range( rel_frame - steps, rel_frame - step_inc, step_inc) ] # find the position of the track at each time for which a tail point should be included (this is determined by steps, which determines how many timesteps back (with each frame = 1 time step) the tail should represent, and step_inc, which determines the interval of timesteps for which a tail point is actually printed)\n",
    "\n",
    "                                                    interp_pos = np.zeros((len(range(step_inc, len(pos)*interpolation_factor-interpolation_factor, 1)),2)) # sets up a two column array that will be filled with the interpolation tail points to print\n",
    "\n",
    "                                                    interp = interpolate.interp1d(range(step_inc, len(pos)*interpolation_factor, step_inc*interpolation_factor), pos[:,0], kind = \"linear\") # do a linear interpolation on the x coordinates of the previous positions of this track to be used in the tail that were extracted above\n",
    "\n",
    "                                                    interp_pos[:,0] = gaussian_filter1d(interp(range(step_inc, len(pos)*interpolation_factor-interpolation_factor, 1)),interpolation_factor) # set the x coordinates within the array to be printed equal to the linear interpolation (with some guassian filtering?)\n",
    "\n",
    "                                                    interp = interpolate.interp1d(range(step_inc, len(pos)*interpolation_factor, step_inc*interpolation_factor), pos[:,1], kind = \"linear\") # do a linear interpolation on the y coordinates of the previous positions of this track to be used in the tail that were extracted above\n",
    "\n",
    "                                                    interp_pos[:,1] = gaussian_filter1d(interp(range(step_inc, len(pos)*interpolation_factor-interpolation_factor, 1)),interpolation_factor) # set the y coordinates within the array to be printed equal to the linear interpolation (with some guassian filtering?)\n",
    "\n",
    "                                                    size_vec = np.power(2,np.linspace(0.5,circle_radius,len(range(0,len(interp_pos),1)))) # make an array of the size that each point in the tail should be printed as, with earlier points being printed smaller\n",
    "\n",
    "                                                    for inc in range(0,len(interp_pos),1): # loop through the indices of the coordinates of the tail points. For each tail point...\n",
    "\n",
    "                                                        pos_loc = interp_pos[ inc ] # extract the coordinate of the point\n",
    "\n",
    "                                                        distance = calc_distance(pos_loc, center_pos) # calculate the distance between this tail point and the current location of the track\n",
    "                                                        # if distance < circle_radius:\n",
    "                                                        #     continue\n",
    "                                                        if not np.isnan( np.sum( pos_loc ) ):\n",
    "                                                            \n",
    "                                                            cv2.circle(im, (int(pos_loc[0]), int(pos_loc[1])), int(np.round(size_vec[inc])), colors[track_ind], -1) # print the tail point at these coordinates, with appropriate size according to that calculated above\n",
    "\n",
    "                                video_writer.write( im ) # write the printed image with the tracks on it to the video\n",
    "\n",
    "                            video_writer.release() # release the video\n",
    "\n",
    "                            files_in_directory = os.listdir(metadata.folder_images) # make a list of the image files of the frames from the input video\n",
    "\n",
    "                            filtered_files = [file for file in files_in_directory if file.endswith('.jpg')] # filter these to make sure they are the images (i.e. end with a 'jpg')\n",
    "\n",
    "                            if delete_frames == True:\n",
    "\n",
    "                                ## reset the frames_already_extracted to false so it decomposes the video into frames for the next video in the loop to be processed\n",
    "                                frames_already_extracted = False\n",
    "                                \n",
    "                                ## delete the image files of the frames of the input video from the folder\n",
    "                                for file in filtered_files:\n",
    "                                    path_to_file = os.path.join(metadata.folder_images, file)\n",
    "                                    os.remove(path_to_file)\n",
    "\n",
    "\n",
    "                            video_out_file_size = os.path.getsize(video_out_file) # calculate the size of the video file that was just produced\n",
    "\n",
    "                            if video_out_file_size > 5e+9: # if the video file is very large...\n",
    "\n",
    "                                os.chdir( metadata.folder_output + new_dir ) # change the working directory to the location of the video that was produced\n",
    "\n",
    "                                subprocess.call( ['ffmpeg', '-i', video_out_file, '-b' ,'800k', 'compressed_' + vid_name + '.mp4'] ) # compress the video\n",
    "\n",
    "                                os.chdir(metadata.folder_code) # change the working directory back to the current directory where the code is\n",
    "\n",
    "                            print('finished video_export ' + vid_name)\n",
    "\n",
    "                        #device = cuda.get_current_device()\n",
    "                        #device.reset()\n",
    "\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                elif entry.is_dir():  #if it is a subfolder\n",
    "                    #print(file.path, 'is dir')\n",
    "\n",
    "                    # Add to paths stack to get to it eventually\n",
    "                    paths.append(entry.path)\n",
    "                    \n",
    "else:\n",
    "    \n",
    "    print( 'input was a file, not a folder' )\n",
    "    \n",
    "    ## set the frame where we want to start the tracking\n",
    "    start = start_init\n",
    "    \n",
    "    ## set the frame where we want to end the tracking\n",
    "    end = end_init\n",
    "    \n",
    "    ## set the range of frames that will be used in the video of the tracking below\n",
    "    sample_range = sample_range_init\n",
    "        \n",
    "    input_video_file = metadata.folder_data_server\n",
    "\n",
    "    ## extract the name of the file from the file path\n",
    "    original_name = input_video_file.split( '/' )[ -1 ]\n",
    "\n",
    "    ## extract the name of the file without the file extension \n",
    "    vid_name = original_name.split( '.' )[ 0 ]\n",
    "    \n",
    "    new_dir = vid_name\n",
    "    \n",
    "    ## you can use another csv here with some metadata in it to check whether the video has baboons in it, and only do tracking if there are baboons in it. For now, just set this to be 1 (treat all videos as if there are baboons in them)\n",
    "    video_has_baboons = 1\n",
    "\n",
    "    if video_has_baboons:\n",
    "\n",
    "        ## save the current time\n",
    "        t = time.time()\n",
    "\n",
    "        ## empty the GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        get_gpu_memory()\n",
    "\n",
    "        ##### prepare to set up the output files for the detections, tracks, and video\n",
    "\n",
    "        ### detections, tracks, and positions will all be added to the output folder (specified above) in a folder structure mirroring the input folder structure; each file will also be labeled with video name that it came from \n",
    "\n",
    "        # the detections, tracks, and positions, and tracking video will be given the name of the video file (with prefex)\n",
    "        detections_file_name = \"detections_\" + vid_name + \".npy\" \n",
    "\n",
    "        ## join the path of the output folder and the detection file name to get the full path to the detection file\n",
    "        detections_file = os.path.join( metadata.folder_output + new_dir , detections_file_name)\n",
    "\n",
    "        ## check if this detection file already exists\n",
    "        detections_exist = os.path.isfile(detections_file)\n",
    "\n",
    "        ## assign the name of the tracks file\n",
    "        tracks_file_name = \"tracks_\" + vid_name + \".npy\" \n",
    "\n",
    "        ## join the path of the output folder and the track file name to get the full path to the track file\n",
    "        tracks_file = os.path.join(metadata.folder_output + new_dir , tracks_file_name)\n",
    "\n",
    "        ## check if this track file already exists\n",
    "        tracks_exist = os.path.isfile(tracks_file)\n",
    "        \n",
    "        video_out_file_name = \"video_\" + vid_name + \".mp4\" \n",
    "        \n",
    "        ## assign the name of the video file for the video with the tracks\n",
    "        video_out_file = os.path.join( metadata.folder_output + new_dir, video_out_file_name )\n",
    "              \n",
    "        ## check if the tracking video file already exists\n",
    "        video_export_exist = os.path.isfile(video_out_file)\n",
    "\n",
    "        ### several lines below are an expansion of this line: video_to_frames(video_path=videopath, frames_dir=(metadata.folder_images), overwrite=True, every=1)\n",
    "\n",
    "\n",
    "        #### the following lines of code extract all the frames from the input video\n",
    "\n",
    "        if frames_already_extracted != True:\n",
    "\n",
    "            ## not really sure what the os.path.join does here, but this just sets up the folder where the images will be extracted to\n",
    "            os.makedirs( ( os.path.join( metadata.folder_images ) ), exist_ok=True )\n",
    "\n",
    "            print('Extracting frames from {}'.format( input_video_file ) )\n",
    "\n",
    "            assert os.path.exists( input_video_file )  # assert the video file exists\n",
    "\n",
    "            # load the VideoReader\n",
    "            vr = VideoReader( input_video_file, ctx = cpu(0) )  # can set to cpu or gpu .. ctx=gpu(0)\n",
    "\n",
    "            if start < 0:  # if start isn't specified lets assume 0\n",
    "                start = 0\n",
    "            if end < 0:  # if end isn't specified assume the end of the video\n",
    "                end = len(vr)\n",
    "\n",
    "            ## you can change every to be image_interval everywhere below if you want\n",
    "            every = image_interval\n",
    "\n",
    "            ## make a list of the frame indices that we will pull out from the video (this is every frame when every = 1 )\n",
    "            frames_list = list(range(start, end, every ) )\n",
    "\n",
    "            ## start a counter of the number of images that are saved (why can't this be inferred from the length of frames_list?)\n",
    "            saved_count = 0\n",
    "\n",
    "            if every > 50 and len(frames_list) < 1000:  # this is faster for every > 25 frames and can fit in memory\n",
    "\n",
    "                frames = vr.get_batch(frames_list).asnumpy() ## pull out all the relevant frames as numpy arrays, in a single operation \n",
    "\n",
    "                for index, frame in zip(frames_list, frames):  # loop through the frames. For each frame...\n",
    "\n",
    "                    save_path = os.path.join( metadata.folder_images, \"{:06d}.jpg\".format(index))  # create the save path. This will save each frame as a JPG with a 6 digit number as a file name (which indicates which frame in the original video it was) in the image folder input above\n",
    "\n",
    "                    if not os.path.exists(save_path) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
    "\n",
    "                        cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # save the extracted image\n",
    "\n",
    "                        saved_count += 1  # increment our counter by one\n",
    "\n",
    "            else:  # this is faster for every <25 and consumes small memory\n",
    "\n",
    "                try:\n",
    "                    \n",
    "                    for index in frames_list:  # lets loop through the frames until the end\n",
    "\n",
    "                        ## why can't the line above just read: for index in frames_list: ?? (it was for index in range( start, end )  Then we don't have to set the if statement below\n",
    "\n",
    "                        frame = vr[ index ]  # read an image from the capture\n",
    "\n",
    "                        # this shouldn't be needad anymore with my adjustment above... if index % every == 0:  # if this is a frame we want to write out based on the 'every' argument\n",
    "\n",
    "                        save_path = os.path.join( metadata.folder_images, \"{:06d}.jpg\".format(index))  # create the save path. This will save each frame as a JPG with a 6 digit number as a file name (which indicates which frame in the original video it was) in the image folder input above\n",
    "\n",
    "                        if not os.path.exists( save_path ) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
    "\n",
    "                            cv2.imwrite(save_path, cv2.cvtColor(frame.asnumpy(), cv2.COLOR_RGB2BGR))  # save the extracted image\n",
    "\n",
    "                            saved_count += 1  # increment our counter by one\n",
    "                            \n",
    "                except:\n",
    "                    \n",
    "                    frames = vr.get_batch(frames_list).asnumpy() ## pull out all the relevant frames as numpy arrays, in a single operation \n",
    "\n",
    "                    for index, frame in zip(frames_list, frames):  # loop through the frames. For each frame...\n",
    "\n",
    "                        save_path = os.path.join( metadata.folder_images, \"{:06d}.jpg\".format(index))  # create the save path. This will save each frame as a JPG with a 6 digit number as a file name (which indicates which frame in the original video it was) in the image folder input above\n",
    "\n",
    "                        if not os.path.exists(save_path) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
    "\n",
    "                            cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # save the extracted image\n",
    "\n",
    "                            saved_count += 1  # increment our counter by one\n",
    "\n",
    "            print( saved_count )  # and return the count of the images we saved\n",
    "\n",
    "        # training\n",
    "\n",
    "        ###### now run inference on the frames that we have extracted from the video\n",
    "\n",
    "        if not detections_exist: # if the model hasn't already been used to infer detections from the frames of this video\n",
    "\n",
    "            # run_inference(vid_name,1) ## this line is expanded out below\n",
    "\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            from IPython import get_ipython\n",
    "            import torch, torchvision, detectron2\n",
    "            from detectron2.utils.logger import setup_logger\n",
    "            import subprocess as sp, numpy as np, cv2, random, glob, torch, time, pickle, matplotlib.pyplot as plt\n",
    "            from detectron2 import model_zoo\n",
    "            from detectron2.engine import DefaultPredictor\n",
    "            from detectron2.config import get_cfg\n",
    "            from detectron2.utils.visualizer import Visualizer\n",
    "            from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "            from detectron2.modeling import build_model\n",
    "            from detectron2.checkpoint import DetectionCheckpointer\n",
    "            from detectron2.data import detection_utils as utils\n",
    "            from detectron2.data import samplers\n",
    "            from torch.utils.data import Dataset, DataLoader\n",
    "            from read_metadata_HIWI import read_metadata\n",
    "            from tqdm import tqdm\n",
    "            import subprocess\n",
    "\n",
    "\n",
    "            class PredictDataset(Dataset):\n",
    "\n",
    "                def __init__(self, glob_string):\n",
    "                    self.image_files = sorted(glob.glob(glob_string))\n",
    "\n",
    "                def __len__(self):\n",
    "                    return len(self.image_files) // 2\n",
    "\n",
    "                def __getitem__(self, idx):\n",
    "                    image_raw = cv2.imread(self.image_files[(idx * 2)])\n",
    "                    height, width = image_raw.shape[:2]\n",
    "                    image = torch.as_tensor(image_raw.astype('float32').transpose(2, 0, 1)).contiguous()\n",
    "                    image_dict0 = {'image':image,  'height':height,  'width':width,  'file_name':self.image_files[idx * 2]}\n",
    "                    image_raw = cv2.imread(self.image_files[(idx * 2 + 1)])\n",
    "                    height, width = image_raw.shape[:2]\n",
    "                    image = torch.as_tensor(image_raw.astype('float32').transpose(2, 0, 1)).contiguous()\n",
    "                    image_dict1 = {'image':image,  'height':height,  'width':width,  'file_name':self.image_files[idx * 2 + 1]}\n",
    "                    return [image_dict0, image_dict1]\n",
    "\n",
    "\n",
    "            if not os.path.exists( os.path.join(metadata.folder_output, new_dir ) ): # make the output directory if it doesn't already exist\n",
    "                os.makedirs( os.path.join(metadata.folder_output, new_dir ) )\n",
    "\n",
    "            cfg = get_cfg() # obtain detectron2's default configuration\n",
    "\n",
    "            cfg.merge_from_file(metadata.folder_detectron + 'configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml') # load values for the configuration of the neural network\n",
    "\n",
    "            cfg.MODEL.WEIGHTS = metadata.baboon_weights # specifies the model weights from the trained model (specified above)\n",
    "           \n",
    "            cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = SCORE_THRESH_TEST\n",
    "            cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = BATCH_SIZE_PER_IMAGE\n",
    "            cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "            cfg.TEST.DETECTIONS_PER_IMAGE = DETECTIONS_PER_IMAGE\n",
    "            cfg.SOLVER.BASE_LR = BASE_LR\n",
    "            cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "            cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = NMS_THRESH_TEST\n",
    "\n",
    "            dataset = PredictDataset(os.path.join(metadata.folder_images, '*.jpg')) # define the dataset for inference (these are all of the frames that we extracted from the input video above)\n",
    "\n",
    "            model = build_model( cfg ) # builds the model structure, but only fills it with random parameters\n",
    "\n",
    "            _ = model.eval() # unclear exactly what this does\n",
    "\n",
    "            checkpointer = DetectionCheckpointer( model )\n",
    "\n",
    "            _ = checkpointer.load( cfg.MODEL.WEIGHTS ) # loads the model weights from the trained models onto the model structure that is now build\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0) # processes the frames into a format needed by the model for inference\n",
    "\n",
    "            cuda0 = torch.device( 'cuda:0' )\n",
    "\n",
    "            print('started detections extraction')\n",
    "\n",
    "            t = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for batch_num, image_batch in tqdm( enumerate(data_loader) ): # loop through the batches extracted from the input video. (each batch appears to contain 2 frames). For each batch..\n",
    "\n",
    "                    if batch_num >= max_batches: # break the loop after getting to the max batches (specified above)\n",
    "\n",
    "                        break\n",
    "\n",
    "                    for i in range(len(image_batch)): # loop through the frames in the batch. For each frame...\n",
    "\n",
    "                        ## format the frame for cuda (I am guessing?)\n",
    "                        image_batch[i]['image'] = np.squeeze(image_batch[i]['image'])\n",
    "                        image_batch[i]['image'] = image_batch[i]['image'].to(cuda0)\n",
    "                        image_batch[i]['width'] = image_batch[i]['width'].to(cuda0).item()\n",
    "                        image_batch[i]['height'] = image_batch[i]['height'].to(cuda0).item()\n",
    "\n",
    "                    predictions = model( image_batch ) # make predictions on the frames in the batch\n",
    "\n",
    "                    ## predictions is a list of dictionaries. Each dictionary is associated with one frame from the image batch and contains the coordinates of the corners of the bounding boxes of each detected object in that frame (as well as some metadata)                       \n",
    "\n",
    "                    for preds, im_dict in zip(predictions, image_batch): # loop through the frames in the batch and their associated predications. For each frame\n",
    "\n",
    "                        frame_number = os.path.splitext(os.path.basename(im_dict['file_name'][0]))[0] # extracts the base name of the image file of the frame without the file extension. Thus, it extracts just the number associated with the frame (as the names of the image files were just the frame numbers)\n",
    "\n",
    "                        pkl_file_path = os.path.join(metadata.folder_output, new_dir, '{}-predictions.pkl'.format(frame_number)) # assign the file path of the pkl file (which will the detections) to be produced. This gives the name of the pkl file the same number as the image file of the frame\n",
    "\n",
    "                        preds_instance = preds['instances'].to('cpu') # extract the detections from the frame\n",
    "\n",
    "                        ## write a pkl file with the detections from the frame\n",
    "                        with open(pkl_file_path, 'wb') as (out):\n",
    "                            pickle.dump(preds_instance, out)\n",
    "                            out.close()\n",
    "\n",
    "            print('finished detections extraction ' + str(time.time() - t)) # print the time it took to run the inference and extract the detections\n",
    "\n",
    "\n",
    "            pkl_files = sorted(glob.glob(os.path.join(metadata.folder_output, new_dir, '*-predictions.pkl'))) # create a list of all the pkl files (there should be one for each frame, containing all the detections for that frame)\n",
    "\n",
    "            all_detections = [] # instantiate an empty list, which will become a list of dictionaries. Each dictionary will correspond to a frame of the video and will contain the coordinates of the bounding boxes and some metadata (essentially, the same that is included in 'predictions' above, with some reformatting); but this list will, at the end of the loop, contain a dictionary for every frame in the video (or at least all those specified above)\n",
    "\n",
    "            raw_instances = [] # instantiate an empty list, which will become a list of dictionaries that will represent almost exactly those seen in 'predictions' above, without reformatting (just subsetted to 'instances'); but again, this list will have a dictionary for every frame in the video, by the end of the loop\n",
    "\n",
    "            for pkl_file in pkl_files[:]: # loop through the pkl files (each associated with one frame of the video). For each pkl file...\n",
    "\n",
    "                with open(pkl_file, 'rb') as (readfile): # open the pkl file and save the contents (which are predictions[i]['instances'] from above) as 'detections'\n",
    "                    detections = pickle.load(readfile)\n",
    "\n",
    "                detection_dict = detections.get_fields() # create a dictionary with the detections from this frame\n",
    "\n",
    "                ## reformat the detections\n",
    "                detection_dict['pred_boxes'] = detection_dict['pred_boxes'].tensor.numpy()\n",
    "                detection_dict['scores'] = detection_dict['scores'].numpy()\n",
    "                detection_dict['pred_classes'] = detection_dict['pred_classes'].numpy()\n",
    "                detection_dict['image_name'] = os.path.basename( pkl_file ).split('-')[0]\n",
    "\n",
    "                all_detections.append(detection_dict) # add the reformatted detections from this frame to the running list of those for all the frames\n",
    "                raw_instances.append(detections) # add the original detections (not reformatted) from this frame to a running list to those for all the frames\n",
    "\n",
    "            np.save(detections_file, all_detections) # save all the detections from every frame to the detections_file in the output folder\n",
    "\n",
    "            fig = plt.figure( figsize=(24, 28) ) # establish a plot size\n",
    "\n",
    "            detections = np.load( detections_file , allow_pickle=True ) # read the detections file back in (contains all the detections from all frames; this should be the same as 'all_detections' above) \n",
    "\n",
    "            for image_ind in random.sample( range( 0, len( detections ) ), 5 ): # for 5 random frames in the video (well, of the frames that were predicted on)....\n",
    "\n",
    "                img = plt.imread( metadata.folder_images + detections[image_ind]['image_name'] + '.jpg' ) # prepare to plot the frame (using the 'image_name' field in 'detections' to find the corresponding frame) \n",
    "\n",
    "                plt.imshow(img) # plot the frame\n",
    "\n",
    "                ax = plt.gca() # get current axes\n",
    "\n",
    "                for item in range(0, len(detections[image_ind]['pred_boxes'])): # loop through the predictions/detections (i.e. bounding boxes) for this frame. For each detection...\n",
    "\n",
    "                    ## extract the coordinates of the corners of the bounding box\n",
    "                    x1 = detections[image_ind]['pred_boxes'][item][0]\n",
    "                    x2 = detections[image_ind]['pred_boxes'][item][2]\n",
    "                    y1 = detections[image_ind]['pred_boxes'][item][1]\n",
    "                    y2 = detections[image_ind]['pred_boxes'][item][3]\n",
    "\n",
    "                    scoretext = str( '{0:.2g}'.format(detections[image_ind]['scores'][item]) ) # extract the score associated with each bounding box and make it a string\n",
    "                    wid = x2 - x1 # calculate the width of the bounding box\n",
    "                    hei = y2 - y1 # calculate the height of the bounding box\n",
    "                    rect = plt.Rectangle((x1, y1), wid, hei, linewidth=1, edgecolor='c', facecolor='none') # prepare plot the bounding box as a rectangle on the frame\n",
    "                    ax.add_patch(rect) # plot the bounding box\n",
    "                    ax.annotate(scoretext, (x1, y1), size=20) # add the score associated with the bound box / detection to the plot\n",
    "                    plt.scatter(x=[x1, x2], y=[y1, y2], c='r', s=10) # plot red dots at the corners of the bounding boxes\n",
    "\n",
    "                save_dir = os.path.join( metadata.folder_output, new_dir, 'example_detections/' )\n",
    "\n",
    "                os.makedirs( save_dir, exist_ok = True )\n",
    "\n",
    "                plt.savefig( os.path.join( save_dir, vid_name + '_' + detections[image_ind]['image_name'] + '.jpg' ), bbox_inches='tight') # save the plot in a folder called 'example detections' which will be in the output folder (under the relevant subdirectory, if applicable)\n",
    "\n",
    "                plt.clf() # clear the plot\n",
    "\n",
    "            ## the next few lines just remove all the pkl files. We don't need them anymore now that we have all the detections for all of the frames in a single file\n",
    "            files_in_directory = os.listdir( os.path.join( metadata.folder_output, new_dir ) ) # make a list of all the files in the directory where we were saving the pkl files\n",
    "\n",
    "            filtered_files = [ file_2_rem for file_2_rem in files_in_directory if file_2_rem.endswith('.pkl')] # subst this list to the pkl files\n",
    "\n",
    "            for remove_file in filtered_files: # for each pkl file...\n",
    "                path_to_file = os.path.join(  metadata.folder_output, new_dir, remove_file ) # save the path to the pkl file\n",
    "                os.remove( path_to_file ) # remove the pkl file, using the path above\n",
    "\n",
    "        else: \n",
    "\n",
    "            detections = np.load( detections_file , allow_pickle=True ) # read the detections file back in (contains all the detections from all frames; this should be the same as 'all_detections' above) \n",
    "\n",
    "        # okay decompiling roi_inference.cpython-36.pyc\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ######### the next several lines turn the detections into tracks\n",
    "\n",
    "        temp_time = time.time()\n",
    "        \n",
    "        if not tracks_exist: # if the tracks have not yet been extracted and saved (can add an else statement here printing that the tracks already exist)\n",
    "\n",
    "            print('detections_to_tracks ' + vid_name)\n",
    "\n",
    "            #### this line is expanded below:  detections_to_tracks( vid_name )\n",
    "\n",
    "            import numpy as np, matplotlib.pyplot as plt, matplotlib.cm as cm\n",
    "            from scipy.optimize import linear_sum_assignment\n",
    "            import os, skimage.transform as transform, cv2, sys\n",
    "\n",
    "            sys.path.append( os.path.join( sys.path[0], './functions') )\n",
    "\n",
    "\n",
    "            from koger_tracking import normalize, create_new_track, finalize_track, _get_boxes_center\n",
    "            from koger_tracking import calculate_distances, calculate_active_list\n",
    "            from koger_tracking import calculate_max_distance, calculate_active_list\n",
    "\n",
    "            os.chdir(metadata.folder_code)\n",
    "\n",
    "            ## check to see if a positions file already exists (positions are just the center points of the bounding boxes saved in detections)\n",
    "            positions_file = os.path.join( metadata.folder_output, new_dir, 'positions_' + vid_name + '.npy' )\n",
    "            positions_exists = os.path.isfile( positions_file )\n",
    "\n",
    "            if positions_exists: # if the positions file already exists\n",
    "\n",
    "                print('Using already computed postions.  Ignoring box files...')\n",
    "\n",
    "                positions = np.load( positions_file, allow_pickle=True ) # load the positions file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for dict_ind, detection_dict in enumerate(detections): # loop through the frames. Just a reminder that detections is a list of dictionaries. Each dictionary corresponds to one frame in the video and contains the detections (bounding boxes) and some metadata for that frame. For each frame...\n",
    "\n",
    "                scores = detection_dict['scores'] # save the scores associated with each bounding box/detection in this frame\n",
    "\n",
    "                if positions_exists: # if the already positions exist...\n",
    "                    positions[dict_ind] = positions[dict_ind][np.where(scores > score_thresh)]  # remove the positions where the score for the detection is below the threshold\n",
    "\n",
    "                else: # if the predictions don't yet exist...\n",
    "                    detections[dict_ind]['pred_boxes'] = detection_dict['pred_boxes'][np.where(scores > score_thresh)] # remove the bounding boxes/detections where the score is below the threshold\n",
    "\n",
    "                detections[dict_ind]['pred_classes'] = detection_dict['pred_classes'][np.where(scores > score_thresh)] # remove the prediction classes associated with detections whose score was below the threshold\n",
    "\n",
    "                detections[dict_ind]['scores'] = detection_dict['scores'][np.where(scores > score_thresh)] # remove the prediction scores associated with detections whose score was below the threshold\n",
    "\n",
    "\n",
    "            if not positions_exists: # if positions do not yet exist...\n",
    "\n",
    "                positions = [] # instantiate an empty list that will become a list of nx2 arrays. Each array will correspond to one frame and contain the xy coordinates of the center of each bounding box on that frame\n",
    "\n",
    "                for d_dict in detections: # for each dictionary in detections (each frame)...\n",
    "\n",
    "                    boxes = d_dict['pred_boxes'] # extract all the bounding boxes from the frame\n",
    "\n",
    "                    positions.append(_get_boxes_center(boxes)) # append an array of the center points of all the bounding boxes to the ongoing positions list \n",
    "\n",
    "                np.save(positions_file, positions) # save the positions (the list of arrays) to the positions_file\n",
    "\n",
    "            shape = [] # instantiate an empty list that will tell us how many detections were made in each frame\n",
    "\n",
    "            for frame_positions in positions: # loop through the list of position arrays (one array for each frame). For each frame...\n",
    "\n",
    "                shape.append(frame_positions.shape[0]) # find the number of rows in the associated array, and append it to the shape list. This is the number of detections that were made (and that exceeded the threshold score) in that frame\n",
    "\n",
    "            classes_exists = False # this defines whether there are different classes of objects have been specified in training and inference (but I'm not sure what this would actually do for the tracking). This has to remain as 'False' unless the 'classes_list' is defined somewhere\n",
    "\n",
    "\n",
    "            raw_track_list = [] # instantiate an empty list that will eventually be filled with all the tracks from the input video\n",
    "\n",
    "            class_label = None\n",
    "            head_position = None\n",
    "\n",
    "            start_frame = 0 # declare the start frame where to start the tracking. Here, we will start at the first frame passed to the model for inference (the first frame of the video, unless specified otherwise above)\n",
    "            end_frame = len(positions) # declare the end frame where the tracking will end. Here, we will end at the last frame passed to the mode inference (the last frame of the video, unless specified otherwise above)\n",
    "            #     end_frame = 30000 # use this to cut the tracking off at a given frame\n",
    "\n",
    "            # Create initial tracks based on the objects in the first frame\n",
    "\n",
    "            for ind in range(positions[start_frame].shape[0]): # loop through the detections/positions in the starting frame. For each detection....\n",
    "                #each track entry is both an array that store all points added to the track and a value that\n",
    "                #indicates the frame on which the track begins\n",
    "\n",
    "                if classes_exists: ## if different classes have been specified in training and inference\n",
    "                    class_label = classes_list[start_frame][ind] ## hmmm, there's a problem here, because classes_list doesn't exist...??\n",
    "\n",
    "                raw_track_list.append(create_new_track(\n",
    "                    start_frame, positions[start_frame][ind], ind, class_label, head_position, noise=0)) ## this function just puts all of these inputs into a dictionary, that defines the start position and frame of a track. Append this dictionary to the list of dictionaries, with each dictionary representing a track\n",
    "\n",
    "            #try to connect points to the next frame\n",
    "            for frame in range(start_frame + 1, end_frame): # loop through the rest of the frame indices. For each frame index...\n",
    "                #if frame % 1000 == 0:\n",
    "                #    print('frame number', frame, 'processed')\n",
    "\n",
    "                #get tracks that are still active (have been seen within the specified time)\n",
    "                active_list = calculate_active_list(raw_track_list, max_unseen_time, frame) # this function scans through all the tracks and inspects whether their time last seen is less than the max_unseen_time threshold. If it is, then it appends the index of this track to the active track list and returns the active track list at the end.\n",
    "                # the end result of active_list, is therefore a list of integers, indicating the indices of tracks within raw_track_list that are still active\n",
    "\n",
    "                #positions from the next step\n",
    "                positions1 = positions[ frame ] # pull out the positions of detections in this frame \n",
    "\n",
    "\n",
    "                #No existing tracks to connect to\n",
    "                if len(active_list) == 0: # if there are no existing tracks\n",
    "\n",
    "                    #Every point in next frame must start a new track\n",
    "                    if len(positions1) != 0: # if there are detections in this frame\n",
    "\n",
    "                        for position in range(positions1.shape[0]): # loop through the detections on this frame. For each detection/position...\n",
    "\n",
    "                            if classes_exists:\n",
    "                                class_label = classes_list[frame][position]\n",
    "\n",
    "                            raw_track_list.append(create_new_track(frame, positions1[position], position, class_label, head_position)) # start a new track for each detection in this frame\n",
    "\n",
    "                    continue # continue to the next frame at this point\n",
    "\n",
    "                # To keep track of index of new points added for things like \n",
    "                # Keeping track of image locations, remove indexs that are already added\n",
    "                positions1_indexes = np.arange(positions1.shape[0]) # save a 1D array of all integers from 0 to the number of detection in this frame\n",
    "\n",
    "                # This will be changed linking information if points to connect\n",
    "                # If not, will reamin empty\n",
    "                row_ind_full = []\n",
    "\n",
    "                # Make sure there are new points to add\n",
    "                if len(positions1) != 0: # if there are detections in this frame...\n",
    "\n",
    "#                                 this line is expanded below :raw_track_list = calculate_max_distance(raw_track_list, \n",
    "#                                                                         active_list, \n",
    "#                                                                         max_distance_threshold, \n",
    "#                                                                         max_distance_threshold_noise, \n",
    "#                                                                         min_distance_threshold) # this function updates the 'max_distance' value in the raw_track_list dictionary. To be honest, I don't fully understand the max_distance value\n",
    "\n",
    "\n",
    "                    ## the next several lines find the nearest neighbor distances for each of the currently active tracks (using their latest positions). We are trying to find the distance that each track can 'look' for a new detection to add to its track. \n",
    "                    ## This distance is determined by a max distance threshold we set, as well as a the location of the other tracks. A track can't look beyond it's nearest neighbor track for its next detection (actually, it can't look past about the midway -- actually 45% of the way -- between the track and it's nearest neighbor track, is then the point might belong to the other track)\n",
    "\n",
    "                    # only check distances to established tracks\n",
    "                    positions0 = [raw_track_list[active_list[0]]['track'][-1]] # this returns the latest position of the first active track, and makes it the first entry of a list\n",
    "\n",
    "                    if len(active_list) > 1: # if there is more than one active track...\n",
    "\n",
    "                        for track_num in active_list[1:]: # loop through the indices of the tracks that are active (i.e. seen within the threshold amount of time). For each active track...\n",
    "\n",
    "                            if raw_track_list[track_num]['noise'] <= 0: # if the noise parameter in this track is less than or equal to 0.... \n",
    "\n",
    "                                positions0.append(raw_track_list[track_num]['track'][-1]) # append the latest position in this active track to the ongoing list of latest positions of active tracks\n",
    "\n",
    "\n",
    "                    positions0 = np.stack(positions0) # turns the list of 1x2 arrays (xy coordinates for the latest position of each active track) into an nx2 array (n = the number of active tracks in which the noise parameter is not > 0)\n",
    "\n",
    "                    ## this line expanded out below: distance = calculate_distances(positions0, track_list, active_list)\n",
    "\n",
    "                    old_positions = [raw_track_list[track_num]['track'][-1] for track_num in active_list] # extract the last coordinate of the all the active tracks. This will be the same as positions0 unless some of the active tracks had a noise parameter greater than 0 (in which case they will be left out of positions0 but included in old_positions)\n",
    "                    old_positions = np.stack(old_positions) # turns the list of 1x2 arrays (xy coordinates for the latest position of each active track) into an mx2 array ( m = the number of active tracks)\n",
    "\n",
    "\n",
    "                    x_diff = (np.expand_dims(positions0[:, 1], 0) \n",
    "                              - np.expand_dims(old_positions[:, 1], 1)\n",
    "                             ) # find the difference in the x-coordinates between all the points (pairwise)\n",
    "\n",
    "                    y_diff = (np.expand_dims(positions0[:, 0], 0) \n",
    "                              - np.expand_dims(old_positions[:, 0], 1)\n",
    "                             ) # find the difference in the y-coordinates between all the points (pairwise)\n",
    "\n",
    "                    distance = np.sqrt(x_diff ** 2 + y_diff ** 2) # returns an m x n array (with m = the number of active tracks, and n = the number of active tracks with noise parameter not greater than 0), and contains the pairwise distances between the latest points of all active tracks (except for those that exceeded the noise parameter -- see above)\n",
    "\n",
    "                    # closest point will be itself, so make zero distance \n",
    "                    # bigger than other distances\n",
    "                    distance[np.where(distance == 0)] = float(\"inf\") # sets the entries on the diagonal of the array equal to infinity, instead of zero\n",
    "\n",
    "                    # HYPER PARAMETER\n",
    "                    # divide by three because saying that within a \n",
    "                    # third of distance to neighbor much more likely to be\n",
    "                    # you then them\n",
    "                    closest_neighbor = np.min(distance, 1) * .45 # finds the nearest neighbor distance of each active track and multiplies them by the hyperparameter\n",
    "\n",
    "                    # Even if neighbors are all far away, have a max threshold to look for new points \n",
    "                    closest_neighbor[np.where(closest_neighbor > max_distance_threshold)] = max_distance_threshold # set the nearest neighbor distance equal to the max_distance threshold for the active tracks whose nearest neighbor is actually farther away than this threshold \n",
    "                    closest_neighbor[np.where(closest_neighbor < min_distance_threshold)] = min_distance_threshold # set the nearest neighbor distance equal to the min_distance threshold for the active tracks whose nearest neighbor is actually closer than this threshold \n",
    "\n",
    "                    for active_ind, track_num in enumerate(active_list): # loop through the indices of the tracks that are still active. For each active track...\n",
    "\n",
    "                        raw_track_list[track_num]['max_distance'] = closest_neighbor[active_ind]  # set the max_distance field in the active track's dictionary equal to the nearest neighbor distance\n",
    "\n",
    "                        if raw_track_list[track_num]['noise'] > 0: # if the noise field of this active track is greater than 0 (i.e. this track has remained unseen more than it has remained seen since it was first detected)... \n",
    "\n",
    "                            if closest_neighbor[active_ind] > max_distance_threshold_noise: # if the closest neighbor for this active track is greater than the max_distance_noise_threshold...\n",
    "\n",
    "                                raw_track_list[track_num]['max_distance'] = max_distance_threshold_noise # set the max_distance field of this track's dictionary equal to the max_distance_noise_threshold\n",
    "\n",
    "\n",
    "                        ## HYPER PARAMETER\n",
    "                        use_size = False\n",
    "\n",
    "                        ## use_size has been set to False, so I am skipping over this for now (I think it has to do with using the size of the detection (bounding box) to adjust the radius at which we look around for other points to add to the track). Go to Koger functions, calculate_max_distances to find out more...\n",
    "\n",
    "                        if use_size:\n",
    "                            size = raw_track_list[track_num]['size'][-1]\n",
    "                            max_distance = raw_track_list[track_num]['max_distance']\n",
    "                            if size < 30:\n",
    "                                max_distance = np.min([15, max_distance]) \n",
    "                            elif size < 120:\n",
    "                                max_distance = np.min([20, max_distance])\n",
    "                            elif not np.isnan(size):\n",
    "                                if min_distance_big:\n",
    "                                    # Even is points near by, give room to look around\n",
    "                                    max_distance = np.max([min_distance_big, max_distance])\n",
    "\n",
    "                            raw_track_list[track_num]['max_distance'] = max_distance\n",
    "\n",
    "\n",
    "                    #returns an array of shape (len(active_list), positions1.shape[0])\n",
    "                    #row is distance from every new point to last point in row's active list \n",
    "                    ## this line is expanded out below: distance_raw = calculate_distances(positions1, raw_track_list, active_list)\n",
    "        #             if frame > 4000:\n",
    "\n",
    "\n",
    "                    #positions from last step\n",
    "                    old_positions = [raw_track_list[track_num]['track'][-1] for track_num in active_list] # extract the last coordinate of the all the active tracks. This will be the same as positions0 unless some of the active tracks had a noise parameter greater than 0 (in which case they will be left out of positions0 but included in old_positions)\n",
    "                    old_positions = np.stack(old_positions) # turns the list of 1x2 arrays (xy coordinates for the latest position of each active track) into an mx2 array ( m = the number of active tracks)\n",
    "\n",
    "                    x_diff = (np.expand_dims(positions1[:, 1], 0) \n",
    "                              - np.expand_dims(old_positions[:, 1], 1)\n",
    "                             ) # find the difference in the x-coordinates between all current detections and the latest positions of each active track (pairwise)\n",
    "\n",
    "                    y_diff = (np.expand_dims(positions1[:, 0], 0) \n",
    "                              - np.expand_dims(old_positions[:, 0], 1)\n",
    "                             ) # find the difference in the y-coordinates between all current detections and the latest positions of each active track (pairwise)\n",
    "\n",
    "                    distance_raw = np.sqrt(x_diff ** 2 + y_diff ** 2) # calculate the distance between all current detections and the latest positions of each active track (pairwise). Store this as a numpy mxn array (m = the number of active tracks, n = the number of detections on the current frame)\n",
    "\n",
    "                    max_distance = np.zeros_like(distance_raw) # makes an array of 0's with the same dimensions as distance_raw (mxn -- m = the number of active tracks, n = the number of detections on the current frame)\n",
    "                    not_noise_tracks = np.ones(len(active_list)) # makes a 1D array of ones that is the length of the number of active tracks\n",
    "                    track_length = np.zeros(len(active_list)) # makes a 1D array of zeros that is the length of the number of active tracks\n",
    "\n",
    "                    for active_num, track_num in enumerate(active_list): # loop through the indices of the tracks that are active. For each active track...\n",
    "\n",
    "                        max_distance[active_num, :] = raw_track_list[track_num]['max_distance'] # fill the row of the max_distance array corresponding to this active track with the max_distance to look for a detection\n",
    "\n",
    "                        track_length[ active_num ] = frame - raw_track_list[track_num]['first_frame'] # calculate the length of the track and insert it in the position of the 1D array corresponding to this active track\n",
    "\n",
    "                        if raw_track_list[track_num]['noise'] > 0: # if this active track has a noise field greater than 0...\n",
    "\n",
    "                            not_noise_tracks[active_num] = 0 # switch the 1 to a 0 in the 1D array in the position correspondingto this active track\n",
    "\n",
    "                    is_max_distance = distance_raw > max_distance ## create an mxn array of booleans declaring whether the distance from each active track to each current detection is too far to consider each current detection a part of each active track\n",
    "\n",
    "                    new_track = np.all(is_max_distance, 0) # returns 1D array with length equal to the number of new detections. This declares whether each new detection represents a new track (i.e. it is not within range of current active tracks). False implies that it belongs to an active track\n",
    "\n",
    "                    new_track_ind = np.where(new_track)[0] # returns an array of the indices of current detections that represent new tracks\n",
    "\n",
    "                    if np.any(new_track): # if any of the current detections represent new tracks (i.e. there are current detections that too far away from active tracks to belong to them)...\n",
    "\n",
    "                        for t in new_track_ind: # loop through the indices of the current detections that represent new tracks. For each detection that represents a new track...\n",
    "                            if classes_exists:\n",
    "                                class_label = classes_list[frame][t]\n",
    "\n",
    "                            raw_track_list.append(create_new_track(frame, positions1[t], t, class_label, head_position)) # append the position as the start of a new track in the track list\n",
    "\n",
    "                    # Get rid of new points that are too far away and were just added as new tracks\n",
    "                    distance = np.delete(distance_raw, new_track_ind, 1) # removes columns from the pairwise distance matrix associated with positions that were just made into new tracks because they were too far away from existing active tracks  \n",
    "\n",
    "                    if distance.shape[1] > 0: ## if there are current detections that can be assigned to existing tracks...\n",
    "\n",
    "                        # To keep track of index of new points added for things like \n",
    "                        # Keeping track of image locations, remove indexs that are already added\n",
    "                        positions1_indexes = np.delete(positions1_indexes, new_track_ind) # remove the indices of current detections/positions that represented new tracks\n",
    "                        # Remove the new positions corresponding to added tracks\n",
    "                        positions1 = np.delete(positions1, new_track_ind, 0) # remove the current detections/positions that represented new tracks\n",
    "\n",
    "                        #connect the dots from one frame to the next\n",
    "                        row_ind, col_ind = linear_sum_assignment(distance) # find the optimal distribution of each current detection to an active track, such that it decreases the total distance between current detections and the active tracks that they are added to. Save the indices that tell which current detection matches with which active track\n",
    "                        # row_ind is the index of the active track that pairs the corresponding index of the current detection indicated in col_ind\n",
    "                        # i.e. row_ind = [ 1, 3, 5 ], col_ind = [ 0, 2, 1 ] means that the first current detection pairs with the second active track, the second current detection pairs with the sixth active track, and the third current detection pairs with the fourth active track\n",
    "\n",
    "                        row_ind_full = np.arange(len(active_list)) # make a 1D array that goes from 0 to the length of active_list\n",
    "                        col_ind_full = np.empty(len(active_list)) # make a 1D array of zeros that is the length of active_list\n",
    "                        col_ind_full[ : ] = np.nan                                                                        \n",
    "\n",
    "                        # In casese where there are fewer new points than existing tracks\n",
    "                        # some tracks won't get new point. The next several lines will determine which tracks shouldn't get a new point because there is no detection close enough, and which tracks should/shouldn't get a new point when there are two or more tracks competing over the same point\n",
    "                        # so, the next several lines essentially 'edit' the linear sum assignment performed above\n",
    "\n",
    "                        duplicates = [] # instantiate an empty list that will be filled with current detections that need to be duplicated because they are becoming a part of more than one track\n",
    "\n",
    "                        to_delete = [] # instantiate an empty list that will be filled with less competitive track competing for the same current detection as a more competitive track, or tracks that have no current detections close to them\n",
    "\n",
    "                        for r_ind in row_ind_full: # loop through the indices of the active tracks. For each active track...\n",
    "\n",
    "                            if r_ind in row_ind: # if this active track has been paired to a new detection by the linear sum assignment...\n",
    "\n",
    "                                col_ind_full[ r_ind ] = col_ind[ np.where( row_ind == r_ind ) ] # save the index of the current detection that is associated with this active track in the col_ind_full array (this array indicates which current detection each active track pairs with, with 0's representing tracks that don't pair with any current detection). \n",
    "                                # How do we differentiate between 0 meaning it doesn't associate with a detection, and 0 meaning it is associated with the first detection????? Oh, I think they get deleted below\n",
    "\n",
    "\n",
    "                            else: # if this active track has NOT been paired to a new detection by the linear sum assignment...\n",
    "\n",
    "                                if np.min(distance[r_ind]) < raw_track_list[active_list[r_ind]]['max_distance']: # if the closest current detection to this active track is less than the max_distance for this active track (and it just hasn't been paired because it wasn't the least-cost solution determined by linear sum assignment)...\n",
    "\n",
    "                                    # There is a new point within this tracks assignment range\n",
    "                                    duplicates.append(np.argmin(distance[r_ind])) # append the index of the current detection that is the closest to this active track to the duplicates list\n",
    "\n",
    "                                    col_ind_full[r_ind] = duplicates[-1] # save the index of the current detection that is associated with this active track in the col_ind_full array\n",
    "\n",
    "                                else:\n",
    "\n",
    "                                    # This track wasn't assigned a new point and there isn't one close by\n",
    "                                    to_delete.append( r_ind ) # append this index of the active track to the list that will not receive an updated position/detection this frame\n",
    "\n",
    "\n",
    "                        ## one minor issue can arise in the below loop. If the 0 index (i.e. the first current detection) is in duplicates, the loop below will just attribute the first current detection to either the track it is supposed to be paired with via the linear sum assignment, one of the duplicates, or just a track that hasn't yet been paired with a current detection; whichever one is longest\n",
    "                        #I fixed this by changing col_ind_full = np.zeros(...) above to col_ind_full = np.empty(...)\n",
    "\n",
    "                        for duplicate in duplicates: # loop through the indices of the current detections that could be used in more than one track. For each of these detections...\n",
    "\n",
    "                            competing_tracks = np.squeeze(np.argwhere( col_ind_full == duplicate ) ) # create a 1D array of the indices of the active tracks that this detection could become a part of\n",
    "\n",
    "                            dominant_track_ind = np.argmax( track_length[ competing_tracks ] ) # find which of these active tracks is the longest (i.e. started the most frames ago / on the earliest frame)\n",
    "\n",
    "                            to_delete.extend( competing_tracks[ :dominant_track_ind ] ) # add the indices of the competing tracks prior to the index of the dominant track index to the list of tracks that will not receive an updated position/detection this frame \n",
    "\n",
    "                            if dominant_track_ind  < len(competing_tracks): # if the longest track is not the last track in the list of competing tracks...\n",
    "\n",
    "                                to_delete.extend(competing_tracks[dominant_track_ind+1:]) # add the indices of the competing tracks beyond to the index of the longest track to the list of tracks that will not receive an updated position/detection this frame \n",
    "\n",
    "                        to_delete_a  = np.array(to_delete) # turn the list of tracks to delete into an array\n",
    "\n",
    "                        if not len( to_delete_a ) == 0:\n",
    "\n",
    "                            #print( col_ind_full )\n",
    "                            \n",
    "                            ## for the tracks that won't receive a current detection from this frame (because the current detections are too far away or because they were outcompeted by longer running tracks), remove their indices from the pairing of the tracks to the current detections\n",
    "                            col_ind_full = np.delete( col_ind_full, to_delete_a ) \n",
    "                            row_ind_full = np.delete( row_ind_full, to_delete_a )\n",
    "\n",
    "\n",
    "                            #print( col_ind_full)\n",
    "                            \n",
    "                        #see if points got assigned to tracks that are farther than max_threshold_distance\n",
    "                        #This happens when the closer track gets assigned to a different point\n",
    "\n",
    "                        col_ind_full = col_ind_full.astype( int ) ## change the float type array to an array of integers (because we are going to index using this array)\n",
    "\n",
    "                        bad_assign = distance[ row_ind_full, col_ind_full ] > max_distance[:, 0][ row_ind_full ] # save a 1D array of booleans stating whether each pairing of active track to current detection exceeds the maximum distance allowed (based on the threshold and the location of other active tracks)\n",
    "\n",
    "                        not_noise_tracks_used = not_noise_tracks[ row_ind_full ] # declare whether each active track to which a current detection will be appended is noise or not\n",
    "\n",
    "                        if np.any(bad_assign): # if there any instances in which the distance between the active track and the current detection it is paired with exceed the allowable maximum distance...\n",
    "\n",
    "                            bad_assign_points = np.where( bad_assign * not_noise_tracks_used ) # save the indices where the maximum allowable distance is exceeded, and the track was not a noise track\n",
    "\n",
    "                            #Assign multiple tracks to nearby points, in cases where track got assigned to somewhere far away\n",
    "                            #because closer track got assigned to point first\n",
    "                            #this case could come up when two animals get too close so they merge to one point\n",
    "                            col_ind_full[ bad_assign_points ] = np.argmin( distance[ row_ind_full[ bad_assign_points ], : ], 1) # for the active track that was assigned a point far away, assign to the active track the current detection is closest to it\n",
    "\n",
    "                            # There may be some tracks that just don't have any new points nearby.  Now filter those out\n",
    "                            valid_assign = distance[row_ind_full, col_ind_full] <= max_distance[:, 0][row_ind_full] # save a 1D array of booleans declaring whether the distance between the active tracks and the current detections to pair with them are less than or equal to the allowable maximum distance (i.e. did we solve the problem with the previous line? If not, we will not add any current detections to these tracks)\n",
    "\n",
    "                            ## only keep the pairings between active tracks and current detections that are allowable given the maximum allowable distances\n",
    "                            col_ind_full = col_ind_full[ valid_assign ]\n",
    "                            row_ind_full = row_ind_full[ valid_assign ]\n",
    "\n",
    "\n",
    "                active_list = np.array( active_list ) # turn the active list (the list of indices of the raw tracks that are active) into an array\n",
    "\n",
    "\n",
    "                for track_num in range( len( raw_track_list ) ): # loop through the indices of the tracks. For each track...\n",
    "\n",
    "                    if track_num in active_list: # if this track is an active track (i.e. it has been seen since the max_unseen_time)...\n",
    "\n",
    "                        #Case where there are new points in the next frame to add\n",
    "                        if track_num in active_list[ row_ind_full ] and len( positions1 ) != 0: # if there are detections in the current frame, and one of these detections has been paired with this track...\n",
    "\n",
    "                            row_count = np.where( track_num == active_list[row_ind_full] )[0] # locate this active track in row_ind_full, as this index will help us find the current detection that this active track pairs with \n",
    "\n",
    "                            # If this is a refound track, linearly interpolate from when last seen\n",
    "                            if raw_track_list[track_num]['last_frame'] != frame - 1: # if the last frame on which this track was seen was not the previous frame...\n",
    "                                missed_steps = frame - raw_track_list[track_num]['last_frame'] - 1 # calculate how many frames it has been since this track was last seen\n",
    "\n",
    "                                gap_distance = ( positions1[col_ind_full[row_count[0]]] - raw_track_list[track_num]['track'][-1] ) # find the distance between the current detection to add to this track and the last detection from this track\n",
    "\n",
    "                                step_distance = gap_distance / (missed_steps + 1) # divide the total distance between the current detection and the last location of this track by the number of frames since this track has been seen to find out how much the track has moved per frame since it was last seen (i.e. linearly interpolate)\n",
    "\n",
    "                                for step in range(missed_steps): # loop through the frames on which this track hadn't been seen. For each frame...\n",
    "\n",
    "                                    raw_track_list[track_num]['track'][-step - 1] = (\n",
    "                                        raw_track_list[track_num]['track'][-step - 1] + \n",
    "                                        (missed_steps - step) * step_distance) # adjust the track's location on that frame based on the linear interpolation (i.e. to the previous location, add the interpolation step length multiplied by the number of steps)\n",
    "\n",
    "                            raw_track_list[track_num]['track'].append(positions1[col_ind_full[row_count[0]]]) # add the current detection that is paired with this active track to the track\n",
    "\n",
    "                            raw_track_list[track_num]['pos_index'].append(positions1_indexes[col_ind_full[row_count[0]]]) # add the index of the current detection that is paired with this active track to the track metadata (I don't fully understand the importance of this)\n",
    "\n",
    "                            raw_track_list[track_num]['last_frame'] = frame # update the last frame that this active track was seen on with the current frame\n",
    "\n",
    "                            if raw_track_list[track_num]['noise'] > 0: # if the track's noise is greater than 0 (i.e. it hasn't yet been confirmed as a true track)....\n",
    "                                raw_track_list[track_num]['noise'] -= 1 #  subtract one from the noise component (i.e. give it a point towards being a real track because it came up again)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        else: # if the track is in the active list, but there is no current detection associated with it\n",
    "\n",
    "                            raw_track_list[track_num]['track'].append(raw_track_list[track_num]['track'][-1]) # duplicate the previous location of the track (i.e. act as though the track has not moved from the last frame to this frame)\n",
    "\n",
    "                            raw_track_list[track_num]['pos_index'].append(np.nan) # add a NaN to the pos_index field of the track metadata (because no current detection as actually added to the track)\n",
    "                            # isn't a confirmed real track yet\n",
    "                            if raw_track_list[track_num]['noise'] > 0: # if the noise of the track is greater than 0 (i.e. it is not a confirmed track yet)...\n",
    "                                raw_track_list[track_num]['noise'] += 1 # add one to the noise component of the track (i.e. dock it a point away from being a real track, because we didn't see it in this frame)\n",
    "\n",
    "                # Add new tracks for new points that weren't added to existing tracks but weren't far enough away before to aleady get a new track\n",
    "                if \"distance\" in locals():\n",
    "                    if distance.shape[0] < distance.shape[1]: # if there are more current detections than active tracks...\n",
    "\n",
    "                        # There are possible new points\n",
    "                        for pos1_ind in range(positions1.shape[0]): # loop through the current detections. For each detection...\n",
    "\n",
    "                            if pos1_ind in col_ind_full: # if this detection is in the list of indices that have already been associated with an existing track...\n",
    "                                # This point was already added to an existing track\n",
    "                                continue # move onto the next detection\n",
    "\n",
    "                            # Only add points that aren't too close to existing tracks \n",
    "                            if np.min(distance_raw[:, positions1_indexes[pos1_ind]]) > min_new_track_distance: # if the distance from this detection to the closest active track is not below the threshold distance to be considered a new track...\n",
    "                                # This new point isn't too close to existing tracks\n",
    "\n",
    "                                if classes_exists:\n",
    "\n",
    "                                    class_label = classes_list[frame][positions1_indexes[pos1_ind]]\n",
    "\n",
    "                                raw_track_list.append(create_new_track(\n",
    "                                    frame, positions1[pos1_ind], positions1_indexes[pos1_ind], class_label, head_position)) # make this detection into the start of a new track\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Traverse the list in reverse order so if there are multiple tracks that\n",
    "                # need to be removed the indexing doesn't get messed up \n",
    "                for track_num in range(len(raw_track_list) - 1, -1, -1): # loop through the tracks. For each track...\n",
    "\n",
    "                    if raw_track_list[track_num]['noise'] >= noise_thresh: # if the track's noise score has exceeded 10 #HYPER PARAMETER# (i.e. the net refinding of this track has dipped below -10)...\n",
    "\n",
    "                        del raw_track_list[track_num] # then remove the track from the track list (i.e. delete it for good)\n",
    "\n",
    "\n",
    "\n",
    "            # after all the frames have been looped through and all tracks have been formed...\n",
    "\n",
    "            for track_ind, track in enumerate(raw_track_list): # loop through the tracks. For each track...\n",
    "                raw_track_list[track_ind] = finalize_track(track) # turn the list of 1x2 coordinates in the track into a nx2 numpy array, where n = the number of frames that the track persisted\n",
    "\n",
    "\n",
    "\n",
    "            for track in raw_track_list:\n",
    "                #THIS IS THE PART THAT GETS RID OF THE EXTRA PART OF THE TAIL\n",
    "                #number of extra points at the end of track that were added hoping that the point would reapear \n",
    "                #nearby.  Since the tracking is now finished.  We can now get rid of these extra points tacked on to the end\n",
    "\n",
    "                track['track'] = track['track'][:track['last_frame'] - track['first_frame'] + 1] # only keep the rows of the nx2 array of coordinates up to the point where the track was actually last seen\n",
    "\n",
    "                track['pos_index'] = track['pos_index'][:track['last_frame'] - track['first_frame'] + 1] # only keep the indices of the detections up to the point where the track was actually last seen\n",
    "\n",
    "\n",
    "\n",
    "            print('detections to tracks processed')\n",
    "\n",
    "            ## filter out the tracks that are shorter than the required number of frames\n",
    "            track_lengths = [] # instantiate an empty list that will be filled with the length of each track (in terms of number of frames it persisted)\n",
    "            track_list = [] # instantiate an empty list that will be filled with the tracks to keep\n",
    "\n",
    "            for track_num, track in enumerate(raw_track_list): # loop through the tracks in the raw track list. For each track...\n",
    "\n",
    "                track_length = track['track'].shape[0] # save the length of the track\n",
    "\n",
    "                if track_length >= min_length_threshold: # if the length of the track is greater than the minimum track length threshold\n",
    "\n",
    "                    track_lengths.append(track['track'].shape[0]) # append the length of the track to the track_lengths list\n",
    "\n",
    "                    track_list.append(track) # append the track to the final list of tracks\n",
    "\n",
    "            ## HYPER PARAMTER ''\n",
    "            save = True\n",
    "            if save:\n",
    "\n",
    "                np.save(tracks_file, track_list) # save the track list to the tracks_file, if save = True\n",
    "\n",
    "        # for debugging - detections_to_tracks('GH010368')\n",
    "\n",
    "\n",
    "        print( 'time taken for detection to track processing: ', time.time() - temp_time )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not video_export_exist: # if the video file does not already exists\n",
    "\n",
    "            video_out_file_size = 0 # set the file size equal to zero\n",
    "\n",
    "        else: # if the video file already exists...\n",
    "\n",
    "            video_out_file_size = os.path.getsize( video_out_file ) # save the size of the file\n",
    "\n",
    "        if video_out_file_size < 1000: # if the video file is non-existant or small (i.e. not yet a real video)...\n",
    "\n",
    "            print('video_export ' + vid_name)\n",
    "\n",
    "            ## this line is expanded below: export_tracks(vid_name) #  sample_range = range(6000,6300,1)  \n",
    "\n",
    "            from IPython import get_ipython\n",
    "            import pandas as pd, numpy as np, cv2, os, glob, matplotlib.pyplot as plt\n",
    "            from matplotlib import cm\n",
    "            from read_metadata_HIWI import read_metadata\n",
    "            from tqdm import tqdm\n",
    "            from scipy import interpolate\n",
    "            from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "            os.path.join(metadata.folder_images, '*.jpg') # I don't think this does anything\n",
    "\n",
    "            frame_files = sorted( glob.glob( os.path.join( metadata.folder_images, '*.jpg' ) ) ) # save the names of the frame files that were extracted from the input video above\n",
    "\n",
    "            tracks = np.load( tracks_file, allow_pickle=True ) # load the tracks that we just saved above\n",
    "\n",
    "            im = cv2.imread(frame_files[0]) # read in the first frame of the video\n",
    "\n",
    "            fourcc = (cv2.VideoWriter_fourcc)(*'mp4v') # declare the codec for the video\n",
    "\n",
    "            video_writer = cv2.VideoWriter( video_out_file, fourcc, 23.976, (im.shape[1], im.shape[0]) ) # instantiate the video at the desired file location with the codec declared above. Pull the dimensions for the video from the first frame extracted from the input video\n",
    "\n",
    "\n",
    "            colors = np.random.uniform(size=(len(tracks))) # generate random numbers between 0 and 1 (with uniform probability). There will be as many numbers as there are tracks\n",
    "\n",
    "            colors = [get_color(c) for c in colors] # turn each number into a color\n",
    "\n",
    "            all_tracking = pd.DataFrame({'frame':[],  'id':[],  'x':[],  'y':[]}) # start a dataframe that will be filled with all the location of all the tracks for the video\n",
    "\n",
    "            all_distance = [] # len(frame_files) # instantiate an empty list\n",
    "\n",
    "            ## turn the numpy tracks file into a csv\n",
    "            for track_ind, track in enumerate(tracks): # loop through the tracks. For each track...\n",
    "\n",
    "                track_frames = list( range( track['first_frame'], track['last_frame'] + 1 ) ) # make a list of the frames for which this track persists\n",
    "\n",
    "                to_append = pd.DataFrame( {'frame': pd.Series( track_frames ), 'id': [ track_ind ] * len( track_frames), 'x': pd.Series( track['track'][:,0] ) ,  'y':track['track'][:,1] } ) # make a dataframe of the frame number, the track id, and the xy position of that track id on that frame number \n",
    "\n",
    "                all_tracking = all_tracking.append( to_append, ignore_index = True ) # add the dataframe for this track to the running dataframe containing all tracks\n",
    "\n",
    "            all_tracking[ 'vid_name' ] =  vid_name # add the name of the video as a column (so that we can distinguish between track 0 in on hour and track 0 in )\n",
    "\n",
    "            all_tracking = all_tracking.astype( { 'frame': int, 'id': int, 'x': float, 'y': float } )\n",
    "\n",
    "            all_tracking.to_csv( os.path.join(metadata.folder_output, new_dir, 'tracks_' + vid_name + '.csv'), index = False) # write the csv with all the tracks to a file\n",
    "            \n",
    "            if len(sample_range) == 0: # if the sample_range has been set to the default (i.e. essentially hasn't been defined)\n",
    "\n",
    "                sample_range = range(1, len(frame_files), 1) # set the range equal to the entire input video\n",
    "\n",
    "            for frame_ind in tqdm(sample_range): # loop through the frames of the input video. For each frame...\n",
    "\n",
    "                im = cv2.imread(frame_files[frame_ind]) # read the frame in using cv2\n",
    "\n",
    "                for track_ind, track in enumerate(tracks): # loop through the tracks. For each track...\n",
    "\n",
    "                    rel_frame = frame_ind - track['first_frame'] # find the number of this frame compared to the frame on which the track starts from\n",
    "\n",
    "                    if rel_frame >= 0 and track['last_frame'] > frame_ind: # if the current frame comes after the track has started and before the track has ended...\n",
    "\n",
    "                        center_pos = track['track'][rel_frame] # find and save the position of the track on this frame\n",
    "\n",
    "                        if not np.isnan( np.sum( center_pos ) ):                        \n",
    "\n",
    "                            cv2.circle(im, (int(center_pos[0]), int(center_pos[1])), circle_radius, colors[track_ind], -1) # print a circle on the frame representing the center position of the current detection (i.e. the location of the track at this frame)\n",
    "                            cv2.putText(im, str(track_ind), (int(center_pos[0]), int(center_pos[1])), 0, 1, colors[track_ind],3) # label the location of the current detection of the track with the index of the track\n",
    "\n",
    "                            if with_tail: # if there should be a tail printed on the frame showing where the track had been in previous frames...\n",
    "\n",
    "                                if rel_frame > steps: # if the number of frames from the start of the track to the current frame are greater than the number of steps to include in the tail...\n",
    "\n",
    "                                    pos = track['track'][ range( rel_frame - steps, rel_frame - step_inc, step_inc) ] # find the position of the track at each time for which a tail point should be included (this is determined by steps, which determines how many timesteps back (with each frame = 1 time step) the tail should represent, and step_inc, which determines the interval of timesteps for which a tail point is actually printed)\n",
    "\n",
    "                                    interp_pos = np.zeros((len(range(step_inc, len(pos)*interpolation_factor-interpolation_factor, 1)),2)) # sets up a two column array that will be filled with the interpolation tail points to print\n",
    "\n",
    "                                    interp = interpolate.interp1d(range(step_inc, len(pos)*interpolation_factor, step_inc*interpolation_factor), pos[:,0], kind = \"linear\") # do a linear interpolation on the x coordinates of the previous positions of this track to be used in the tail that were extracted above\n",
    "\n",
    "                                    interp_pos[:,0] = gaussian_filter1d(interp(range(step_inc, len(pos)*interpolation_factor-interpolation_factor, 1)),interpolation_factor) # set the x coordinates within the array to be printed equal to the linear interpolation (with some guassian filtering?)\n",
    "\n",
    "                                    interp = interpolate.interp1d(range(step_inc, len(pos)*interpolation_factor, step_inc*interpolation_factor), pos[:,1], kind = \"linear\") # do a linear interpolation on the y coordinates of the previous positions of this track to be used in the tail that were extracted above\n",
    "\n",
    "                                    interp_pos[:,1] = gaussian_filter1d(interp(range(step_inc, len(pos)*interpolation_factor-interpolation_factor, 1)),interpolation_factor) # set the y coordinates within the array to be printed equal to the linear interpolation (with some guassian filtering?)\n",
    "\n",
    "                                    size_vec = np.power(2,np.linspace(0.5,circle_radius,len(range(0,len(interp_pos),1)))) # make an array of the size that each point in the tail should be printed as, with earlier points being printed smaller\n",
    "\n",
    "                                    for inc in range(0,len(interp_pos),1): # loop through the indices of the coordinates of the tail points. For each tail point...\n",
    "\n",
    "                                        pos_loc = interp_pos[ inc ] # extract the coordinate of the point\n",
    "\n",
    "                                        distance = calc_distance(pos_loc, center_pos) # calculate the distance between this tail point and the current location of the track\n",
    "                                        # if distance < circle_radius:\n",
    "                                        #     continue\n",
    "                                        \n",
    "                                        if not np.isnan( np.sum( pos_loc ) ):\n",
    "\n",
    "                                            cv2.circle(im, (int(pos_loc[0]), int(pos_loc[1])), int(np.round(size_vec[inc])), colors[track_ind], -1) # print the tail point at these coordinates, with appropriate size according to that calculated above\n",
    "\n",
    "                video_writer.write( im ) # write the printed image with the tracks on it to the video\n",
    "\n",
    "            video_writer.release() # release the video\n",
    "\n",
    "            files_in_directory = os.listdir(metadata.folder_images) # make a list of the image files of the frames from the input video\n",
    "\n",
    "            filtered_files = [file for file in files_in_directory if file.endswith('.jpg')] # filter these to make sure they are the images (i.e. end with a 'jpg')\n",
    "\n",
    "            if delete_frames == True:\n",
    "\n",
    "                ## delete the image files of the frames of the input video from the folder\n",
    "                for file in filtered_files:\n",
    "                    path_to_file = os.path.join(metadata.folder_images, file)\n",
    "                    os.remove(path_to_file)\n",
    "\n",
    "\n",
    "            video_out_file_size = os.path.getsize(video_out_file) # calculate the size of the video file that was just produced\n",
    "\n",
    "            if video_out_file_size > 5e+9: # if the video file is very large...\n",
    "\n",
    "                os.chdir( metadata.folder_output + new_dir ) # change the working directory to the location of the video that was produced\n",
    "\n",
    "                subprocess.call( ['ffmpeg', '-i', video_out_file, '-b' ,'800k', 'compressed_' + vid_name + '.mp4'] ) # compress the video\n",
    "\n",
    "                os.chdir(metadata.folder_code) # change the working directory back to the current directory where the code is\n",
    "\n",
    "            print('finished video_export ' + vid_name)\n",
    "\n",
    "        #device = cuda.get_current_device()\n",
    "        #device.reset()\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### move the txt files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
